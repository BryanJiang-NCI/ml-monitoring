"""
Spark Streaming Inference (AutoEncoder + TF-IDF + PyTorch)
==========================================================
‚úÖ Âä†ËΩΩ TF-IDF ÂêëÈáèÂô® + AutoEncoder Ê®°ÂûãÔºàPyTorchÔºâ
‚úÖ ÂØπÊØèÊù°Êó•ÂøóÊûÑÂª∫ËØ≠‰πâÂè• ‚Üí ÂêëÈáèÂåñ ‚Üí ËÆ°ÁÆóÈáçÂª∫ËØØÂ∑Æ
‚úÖ Âä®ÊÄÅÂä†ËΩΩËÆ≠ÁªÉÈòàÂÄº threshold.pkl
‚úÖ ËæìÂá∫ JSONÔºöprediction / reconstruction_error / threshold / debug_text
‚úÖ Êó†ÈîÅ„ÄÅÂÆâÂÖ®„ÄÅÂÖºÂÆπ Spark Structured Streaming
==========================================================
"""

import os
import json
import joblib
import torch
import torch.nn as nn
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf
from pyspark.sql.types import StringType

# ==========================================================
# üß© Step 0. Ë∑ØÂæÑÈÖçÁΩÆ
# ==========================================================
MODEL_DIR = "/opt/spark-apps/models/prediction_model"
MODEL_FILE = os.path.join(MODEL_DIR, "autoencoder.pth")
VEC_FILE = os.path.join(MODEL_DIR, "tfidf.pkl")
SCALER_FILE = os.path.join(MODEL_DIR, "scaler.pkl")
THRESH_FILE = os.path.join(MODEL_DIR, "threshold.pkl")

KAFKA_BOOTSTRAP = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"

# ==========================================================
# üß© Step 1. Âä†ËΩΩÊ®°ÂûãÁªÑ‰ª∂
# ==========================================================
print(f"üöÄ Loading model and components from {MODEL_DIR} ...")

vectorizer = joblib.load(VEC_FILE)
scaler = joblib.load(SCALER_FILE)
threshold = joblib.load(THRESH_FILE) if os.path.exists(THRESH_FILE) else 0.01
input_dim = len(vectorizer.get_feature_names_out())


# ÂÆö‰πâ‰∏éËÆ≠ÁªÉ‰∏ÄËá¥ÁöÑ AutoEncoder ÁªìÊûÑÔºà‚ö†Ô∏è Êó† SigmoidÔºâ
class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(AutoEncoder, self).__init__()
        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(hidden_dim, input_dim))  # Linear ËæìÂá∫

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


model = AutoEncoder(input_dim=input_dim, hidden_dim=128)
model.load_state_dict(torch.load(MODEL_FILE, map_location="cpu"))
model.eval()

print(f"‚úÖ AutoEncoder loaded (input_dim={input_dim}, threshold={threshold:.6f})")

# ==========================================================
# üß© Step 2. Â≠óÊÆµÂÆö‰πâ‰∏éÊñáÊú¨ÊûÑÂª∫ÂáΩÊï∞
# ==========================================================
cols_keep = [
    "source_type",
    "ingest_time",
    "access_client",
    "access_method",
    "access_uri",
    "access_status",
    "response_size",
    "request_time",
    "http_referer",
    "user_agent",
    "error_message",
    "error_client",
    "error_server",
    "error_request",
    "error_host",
    "syslog_message",
    "syslog_identifier",
    "syslog_priority",
    "syslog_facility",
    "host",
    "gh_status",
    "gh_conclusion",
    "gh_head_branch",
    "commit_message",
    "cmdb_service",
    "cmdb_domain",
    "cmdb_owner",
    "cmdb_language",
    "cmdb_deployment_env",
    "metric_name",
    "metric_value",
]


def safe_text(x):
    if x is None or str(x).strip() == "":
        return "[NO_DATA]"
    return str(x)


def numeric_to_text(name, val):
    try:
        val = float(val)
    except Exception:
        return f"{name} unknown"
    if val > 0.9:
        return f"{name} high"
    elif val > 0.5:
        return f"{name} medium"
    else:
        return f"{name} normal"


def build_semantic_sentence(row: dict):
    parts = []
    for c in cols_keep:
        val = row.get(c)
        if c in ["metric_value", "access_status", "response_size", "request_time"]:
            val = numeric_to_text(c, val)
        else:
            val = safe_text(val)
        parts.append(f"{c}: {val}")
    return ". ".join(parts)


# ==========================================================
# üß© Step 3. ÂÆûÊó∂Êé®ÁêÜÂáΩÊï∞ÔºàÂü∫‰∫é PyTorchÔºâ
# ==========================================================
def predict_json(row_json: str):
    try:
        data = json.loads(row_json)
        text = build_semantic_sentence(data)

        # ÂêëÈáèÂåñ & Ê†áÂáÜÂåñ
        # X_vec = vectorizer.transform([text]).toarray()
        # X_scaled = scaler.transform(X_vec)
        # X_tensor = torch.tensor(X_scaled, dtype=torch.float32)

        X_vec = vectorizer.transform([text])  # ‰∏çËΩ¨ array
        X_scaled = scaler.transform(X_vec)  # ‰øùÁïôÁ®ÄÁñèÁªìÊûÑ
        X_tensor = torch.tensor(X_scaled.toarray(), dtype=torch.float32)

        # ÈáçÊûÑËØØÂ∑Æ
        with torch.no_grad():
            reconstructed = model(X_tensor)
            mse = torch.mean((X_tensor - reconstructed) ** 2, dim=1).item()

        # Ê†πÊçÆÈòàÂÄºÂà§Êñ≠
        if mse > threshold * 1.5:
            label = "high_anomaly"
        elif mse > threshold:
            label = "low_anomaly"
        else:
            label = "normal"

        return json.dumps(
            {
                "prediction": label,
                "reconstruction_error": round(mse, 6),
                "threshold": round(threshold, 6),
                "source_type": data.get("source_type", "unknown"),
                "debug_text": text[:160],
            }
        )
    except Exception as e:
        return json.dumps({"prediction": "error", "error": str(e)})


predict_udf = udf(predict_json, StringType())

# ==========================================================
# üß© Step 4. ÂàùÂßãÂåñ Spark
# ==========================================================
spark = (
    SparkSession.builder.appName("AutoEncoder_TFIDF_Streaming_Inference")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")
print("‚úÖ Spark initialized.")

# ==========================================================
# üß© Step 5. Kafka Êï∞ÊçÆÊ∫ê
# ==========================================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

df_base = df_kafka.selectExpr("CAST(value AS STRING) as message")

# ==========================================================
# üß© Step 6. Êé®ÁêÜ‰∏éËæìÂá∫
# ==========================================================
df_pred = df_base.withColumn("result_json", predict_udf(col("message")))
df_out = df_pred.select(col("message"), col("result_json"))

query = (
    df_out.writeStream.outputMode("append")
    .format("console")
    .option("truncate", False)
    .option("numRows", 5)
    .start()
)

query.awaitTermination()
