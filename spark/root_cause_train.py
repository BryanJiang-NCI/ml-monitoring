"""
root_cause_train.py
====================
Train Root Cause Correlation Model
----------------------------------
è¾“å…¥:
 - feedback_samples.jsonl (å†å²ç¡®è®¤çš„å¼‚å¸¸æ ·æœ¬)
 - cmdb_entities.jsonl (ç³»ç»Ÿä¸­æœåŠ¡/ç»„ä»¶/ä¸»æœºçš„CMDBæ•°æ®)
è¾“å‡º:
 - ä¿å­˜åˆ° models/root_cause_model
"""

import os
import json
import pandas as pd
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader

BASE_DIR = "/opt/spark/work-dir"
MODEL_SAVE_DIR = f"{BASE_DIR}/models/root_cause_model"
FEEDBACK_FILE = f"{BASE_DIR}/data/feedback_samples.jsonl"
CMDB_FILE = f"{BASE_DIR}/data/cmdb.jsonl"
MODEL_NAME = "all-MiniLM-L12-v2"

os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# Step 1ï¸âƒ£ è¯»å–æ•°æ®
feedback = pd.read_json(FEEDBACK_FILE, lines=True)
cmdb = pd.read_json(CMDB_FILE, lines=True)

# ä»…ä¿ç•™å¸¦ label çš„æ ·æœ¬
feedback = feedback.dropna(subset=["semantic_text", "feedback_label"])
print(f"âœ… Loaded {len(feedback)} labeled feedback samples.")

# Step 2ï¸âƒ£ ç»„åˆè®­ç»ƒæ ·æœ¬
train_samples = []
for _, row in feedback.iterrows():
    train_samples.append(
        InputExample(texts=[row["semantic_text"], row["feedback_label"]])
    )

# Step 3ï¸âƒ£ æ¨¡å‹åˆå§‹åŒ–
encoder = SentenceTransformer(MODEL_NAME)
train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=8)
train_loss = losses.MultipleNegativesRankingLoss(encoder)

# Step 4ï¸âƒ£ è®­ç»ƒ
print("ğŸš€ Training root cause model...")
encoder.fit(
    train_objectives=[(train_dataloader, train_loss)], epochs=3, warmup_steps=50
)
encoder.save(MODEL_SAVE_DIR)
print(f"âœ… Root Cause Model saved to {MODEL_SAVE_DIR}")
