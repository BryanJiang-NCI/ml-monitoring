"""
Incremental (Feedback) Training Script for AutoEncoder
======================================================
âœ… è¯»å– feedback_samples.jsonlï¼ˆå¢é‡æ•°æ®ï¼‰
âœ… åŠ è½½å·²ä¿å­˜æ¨¡å‹ä¸æ ‡å‡†åŒ–å™¨
âœ… å¾®è°ƒ AutoEncoder æ¨¡å‹ï¼ˆ2â€“3 ä¸ª epochï¼‰
âœ… é‡æ–°è®¡ç®— MSE ä¸åŠ¨æ€é˜ˆå€¼
âœ… ä¿å­˜æ›´æ–°åçš„æ¨¡å‹ä¸é˜ˆå€¼
âœ… æ¸…ç©º feedback æ–‡ä»¶
======================================================
"""

import os
import torch
import joblib
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from semantic_train import AutoEncoder

# ==========================================================
# è·¯å¾„é…ç½®ï¼ˆä¸ä¸»æ¨¡å‹ä¿æŒä¸€è‡´ï¼‰
# ==========================================================
BASE_DIR = "/opt/spark/work-dir"
MODEL_DIR = f"{BASE_DIR}/models/prediction_model"
FEEDBACK_FILE = f"{BASE_DIR}/data/feedback_samples.jsonl"
SCALER_FILE = f"{MODEL_DIR}/scaler.pkl"
MODEL_FILE = f"{MODEL_DIR}/autoencoder.pth"
THRESH_FILE = f"{MODEL_DIR}/threshold.pkl"
MODEL_NAME = "all-MiniLM-L12-v2"
hidden_dim = 64

# ==========================================================
# Step 1. åŠ è½½å¢é‡æ•°æ®
# ==========================================================
if not os.path.exists(FEEDBACK_FILE) or os.path.getsize(FEEDBACK_FILE) == 0:
    print("âš ï¸ No new feedback samples found. Skip retraining.")
    exit(0)

df = pd.read_json(FEEDBACK_FILE, lines=True)
if df.empty:
    print("âš ï¸ Feedback file is empty. Skip retraining.")
    exit(0)

texts = df["semantic_text"].tolist()
print(f"ğŸ“¦ Loaded {len(texts)} new feedback samples.")

# ==========================================================
# Step 2. ç¼–ç ä¸æ ‡å‡†åŒ–
# ==========================================================
encoder = SentenceTransformer(MODEL_NAME)
scaler = joblib.load(SCALER_FILE)

X = encoder.encode(texts)
X_scaled = scaler.transform(X).astype(np.float32)
X_tensor = torch.tensor(X_scaled)

# ==========================================================
# Step 3. åŠ è½½æ—§æ¨¡å‹
# ==========================================================
model = AutoEncoder(input_dim=X_tensor.shape[1], hidden_dim=hidden_dim)
model.load_state_dict(torch.load(MODEL_FILE))
model.encoder[2].p = 0.0
model.train()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
X_tensor = X_tensor.to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
criterion = torch.nn.MSELoss()

# ==========================================================
# Step 4. å¾®æ‰¹å¢é‡è®­ç»ƒ
# ==========================================================
EPOCHS = 3
print(f"ğŸš€ Starting incremental fine-tuning for {EPOCHS} epochs...")
for epoch in range(EPOCHS):
    optimizer.zero_grad()
    recon = model(X_tensor)
    loss = criterion(recon, X_tensor)
    loss.backward()
    optimizer.step()
    print(f"Epoch [{epoch+1}/{EPOCHS}] - Loss: {loss.item():.6f}")

# ==========================================================
# Step 5. æ›´æ–°é˜ˆå€¼å¹¶ä¿å­˜æ¨¡å‹
# ==========================================================
model.eval()
with torch.no_grad():
    reconstructed = model(X_tensor)
    mse = torch.mean((X_tensor - reconstructed) ** 2, dim=1).cpu().numpy()

threshold = float(np.percentile(mse, 97.5))
mean_mse = float(np.mean(mse))
print(f"ğŸ“Š Computed 97.5th percentile threshold: {threshold:.6f}")
print(f"ğŸ“ˆ Mean MSE after incremental training: {mean_mse:.6f}")

# torch.save(model.state_dict(), MODEL_FILE)
# joblib.dump(threshold, THRESH_FILE)

print("ğŸ’¾ Incremental model and threshold updated successfully.")

# ==========================================================
# Step 6. æ¸…ç©ºåé¦ˆæ–‡ä»¶
# ==========================================================
# open(FEEDBACK_FILE, "w").close()
# print("ğŸ§¹ Feedback file cleared. Incremental retraining complete.\n")

# ==========================================================
# âœ… End
# ==========================================================
print("âœ… Incremental AutoEncoder fine-tuning completed.")
