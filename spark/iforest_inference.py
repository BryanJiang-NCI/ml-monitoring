"""
Spark Streaming Inference (Semantic Pipeline + Isolation Forest)
================================================================
‚úÖ ‰∏éËØ≠‰πâÂêëÈáèËêΩÂú∞ÈÄªËæëÂÆåÂÖ®‰∏ÄËá¥ÔºàÂêåÊ†∑ÁöÑ json ‚Üí semantic_text ‚Üí encodeÔºâ
‚úÖ ‰ªé Kafka ÂÆûÊó∂ËØªÂèñ ‚Üí ÂêëÈáèÂåñ ‚Üí ‰ΩøÁî®ËÆ≠ÁªÉÂ•ΩÁöÑ Isolation Forest Êé®ÁêÜ
‚úÖ ÊâìÂç∞È¢ÑÊµãÁªìÊûúÔºàÂèØÈÄâÂêåÊó∂ËêΩÂú∞ parquetÔºâ
‚úÖ ÂºÇÂ∏∏Ê£ÄÊµãÁªìÊûúÂÜôÂÖ•Êñá‰ª∂ (/opt/spark/work-dir/data/anomaly_iforest.jsonl)
================================================================
"""

import os
import json
import joblib
import numpy as np
from datetime import datetime
from sentence_transformers import SentenceTransformer
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, get_json_object
from pyspark.sql.types import StringType

# ==========================================================
# üß© Step 0. Ë∑ØÂæÑÈÖçÁΩÆ
# ==========================================================
KAFKA_SERVERS = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"

BASE_DIR = "/opt/spark/work-dir"
MODEL_DIR = os.path.join(BASE_DIR, "models", "iforest_model")

SCALER_FILE = os.path.join(MODEL_DIR, "scaler.pkl")
MODEL_FILE = os.path.join(MODEL_DIR, "iforest.pkl")
THRESH_FILE = os.path.join(MODEL_DIR, "threshold.pkl")
ANOMALY_LOG_FILE = os.path.join(BASE_DIR, "data/anomaly.jsonl")
MODEL_NAME = "all-MiniLM-L12-v2"

print(f"üöÄ Initializing SentenceTransformer: {MODEL_NAME}")
encoder = SentenceTransformer(MODEL_NAME)

# ==========================================================
# üß© Step 1. Âä†ËΩΩ Isolation Forest Ê®°Âûã‰∏éÊ†áÂáÜÂåñÂô®
# ==========================================================
scaler = joblib.load(SCALER_FILE)
model = joblib.load(MODEL_FILE)
threshold = float(joblib.load(THRESH_FILE))
print(f"‚úÖ IsolationForest loaded (threshold={threshold:.6f})")

# ==========================================================
# üß© Step 2. Spark ÂàùÂßãÂåñ
# ==========================================================
spark = (
    SparkSession.builder.appName("SemanticStreamingInference_IForest")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .config("spark.sql.execution.arrow.pyspark.enabled", True)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")
print("‚úÖ Spark initialized successfully.")

# ==========================================================
# üß© Step 3. ‰ªé Kafka ËØªÂèñ
# ==========================================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_SERVERS)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)
df_raw = df_kafka.selectExpr("CAST(value AS STRING) as message")


# ==========================================================
# üß© Step 4. JSON ‚Üí Semantic SentenceÔºà‰øùÊåÅ‰∏ÄËá¥Ôºâ
# ==========================================================
def json_to_semantic(text):
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "message" in data:
            try:
                msg = json.loads(data["message"])
            except Exception:
                msg = data["message"]
        else:
            msg = data

        parts = []
        if isinstance(msg, dict):
            for k, v in msg.items():
                if any(
                    t in k.lower() for t in ["time", "timestamp", "date", "created_at"]
                ):
                    continue
                parts.append(f"{k} {v}")
        else:
            parts.append(str(msg))
        return " ".join(parts)
    except Exception:
        return "[INVALID_JSON]"


semantic_udf = udf(json_to_semantic, StringType())
df_semantic = df_raw.withColumn("semantic_text", semantic_udf(col("message")))
df_semantic = df_semantic.withColumn(
    "source_type", get_json_object(col("message"), "$.source_type")
)
df_semantic = df_semantic.withColumn(
    "ingest_time", get_json_object(col("message"), "$.timestamp")
)


# ==========================================================
# üß© Step 5. ÂêëÈáèÂåñ + Isolation Forest Êé®ÁêÜ + ÂºÇÂ∏∏ËÆ∞ÂΩï
# ==========================================================
def infer_iforest(text):
    try:
        # --- ËØ≠‰πâÂêëÈáèÂåñ ---
        emb = encoder.encode(text)
        emb_scaled = scaler.transform([emb])

        # --- Isolation Forest È¢ÑÊµã ---
        score = -model.score_samples(emb_scaled)[0]  # Ë∂äÈ´òË∂äÂºÇÂ∏∏
        label = "anomaly" if score > threshold else "normal"
        ratio = score / threshold

        result = {
            "timestamp": datetime.utcnow().isoformat(),
            "semantic_text": text,
            "prediction": label,
            "score": round(score, 6),
            "threshold": round(threshold, 6),
        }

        # --- ÂºÇÂ∏∏ÂÜôÂÖ•Êñá‰ª∂ ---
        if label != "normal":
            with open(ANOMALY_LOG_FILE, "a") as f:
                f.write(json.dumps(result) + "\n")
            print(f"‚ö†Ô∏è Anomaly detected ‚Üí logged to {ANOMALY_LOG_FILE}: {result}")

        return json.dumps(result)
    except Exception as e:
        return json.dumps({"prediction": "error", "error": str(e)})


infer_udf = udf(infer_iforest, StringType())
df_pred = df_semantic.withColumn("result", infer_udf(col("semantic_text")))

# ==========================================================
# üß© Step 6. ËæìÂá∫ÁªìÊûú
# ==========================================================
query_console = (
    df_pred.select("source_type", "ingest_time", "semantic_text", "result")
    .writeStream.outputMode("append")
    .format("console")
    .option("truncate", False)
    .option("numRows", 5)
    .start()
)

print(f"üì° Streaming inference started (Isolation Forest) from topic: {KAFKA_TOPIC}")
query_console.awaitTermination()
