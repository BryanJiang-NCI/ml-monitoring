import os
import json
import re
from glob import glob

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import joblib
import matplotlib.pyplot as plt
from sentence_transformers import SentenceTransformer
from sklearn.metrics import precision_recall_fscore_support

# ==========================
# 0. è·¯å¾„é…ç½®
# ==========================
BASE_DIR = "/opt/spark/work-dir"

# æµ‹è¯•é›†ç›®å½•ï¼ˆä½ ä¹‹å‰ label åçš„è¾“å‡ºç›®å½•ï¼‰
TEST_DIR = os.path.join(BASE_DIR, "data/test_labeled_parquet")

# è¯­ä¹‰æ¨¡å‹
SEM_MODEL_NAME = "all-MiniLM-L12-v2"
SEM_SCALER_PATH = os.path.join(BASE_DIR, "models/prediction_model/scaler.pkl")
SEM_MODEL_PATH = os.path.join(BASE_DIR, "models/prediction_model/autoencoder.pth")
SEM_THRESH_PATH = os.path.join(BASE_DIR, "models/prediction_model/threshold.pkl")

# ç»“æ„åŒ–æ¨¡å‹
STR_MODEL_DIR = os.path.join(BASE_DIR, "models/structured_model")
STR_PREPROC_PATH = os.path.join(STR_MODEL_DIR, "preprocessor.pkl")
STR_MODEL_PATH = os.path.join(STR_MODEL_DIR, "autoencoder.pth")
STR_THRESH_PATH = os.path.join(STR_MODEL_DIR, "threshold.pkl")

# æŒ‡æ ‡è¾“å‡º
METRICS_DIR = os.path.join(BASE_DIR, "metrics")
os.makedirs(METRICS_DIR, exist_ok=True)
CSV_PATH = os.path.join(METRICS_DIR, "eval_metrics.csv")
PLOT_PATH = os.path.join(METRICS_DIR, "model_comparison.png")


# ==========================
# 1. å…¬ç”¨ï¼šAutoEncoder å®šä¹‰
# ==========================
class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2)
        )
        self.decoder = nn.Sequential(nn.Linear(hidden_dim, input_dim))

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


# ==========================
# 2. è¯­ä¹‰é¢„å¤„ç†ï¼šjson_str â†’ semantic_text
#    ä¿æŒä¸ä½ å½“å‰è®­ç»ƒè„šæœ¬ä¸€è‡´ï¼ˆkey=value + å¿½ç•¥ time å­—æ®µï¼‰
# ==========================
def json_to_semantic(text: str) -> str:
    try:
        data = json.loads(text)
        # ä½ çš„æ¶ˆæ¯ç»“æ„ï¼š{"source_type": "...", "timestamp": "...", "message": "..."}
        if isinstance(data, dict) and "message" in data:
            msg = data["message"]
            # message å¤§å¤šæ˜¯ JSON å­—ç¬¦ä¸²
            if isinstance(msg, str):
                try:
                    msg = json.loads(msg)
                except Exception:
                    # ä¸æ˜¯ JSON çš„è¯ï¼Œç›´æ¥æ‹¿åŸæ–‡
                    return str(msg)
        else:
            msg = data

        parts = []
        if isinstance(msg, dict):
            for k, v in msg.items():
                # å’Œè®­ç»ƒæ—¶ä¸€æ ·ï¼šè¿‡æ»¤æ—¶é—´ç›¸å…³å­—æ®µ
                if any(
                    t in k.lower() for t in ["time", "timestamp", "date", "created_at"]
                ):
                    continue
                parts.append(f"{k}={v}")
        else:
            parts.append(str(msg))

        return " ".join(parts)
    except Exception:
        return "[INVALID_JSON]"


# ==========================
# 3. ç»“æ„åŒ–ç‰¹å¾ï¼šjson_str â†’ FEATURE_COLUMNS
#    å®Œå…¨æŒ‰ä½ è´´çš„ Spark ç»“æ„åŒ–è„šæœ¬é€»è¾‘æ¥è§£æ
# ==========================
preprocessor = joblib.load(STR_PREPROC_PATH)
FEATURE_COLUMNS = list(preprocessor.feature_names_in_)


def build_structured_features(df: pd.DataFrame) -> pd.DataFrame:
    rows = []

    for raw in df["json_str"]:
        try:
            d = json.loads(raw)
        except Exception:
            d = {}

        st = d.get("source_type", "")
        if st != "app_container_metrics":
            msg = d.get("message", {})

        if isinstance(msg, str):
            try:
                msg_json = json.loads(msg)
            except Exception:
                # nginx_error è¿™ç§æœ¬èº«æ˜¯çº¯å­—ç¬¦ä¸²æ—¥å¿—
                msg_json = {}
        elif isinstance(msg, dict):
            msg_json = msg
        else:
            msg_json = {}

        # åˆå§‹åŒ–ä¸€è¡Œï¼Œæ‰€æœ‰ FEATURE_COLUMNS å…ˆç½®ç©ºå­—ç¬¦ä¸²
        f = {col: "unknown" for col in FEATURE_COLUMNS}
        f["source_type"] = st

        # --- GitHub Commits ---
        if st == "github_commits":
            f["commit_email"] = msg_json.get("email", "")
            f["commit_author"] = msg_json.get("author", "")
            f["commit_repository"] = msg_json.get("repository", "")

        # --- GitHub Actions ---
        elif st == "github_actions":
            f["action_event"] = msg_json.get("event", "")
            f["action_name"] = msg_json.get("name", "")
            f["action_pipeline_file"] = msg_json.get("pipeline_file", "")
            f["action_build_branch"] = msg_json.get("build_branch", "")
            f["action_status"] = msg_json.get("status", "")
            f["action_conclusion"] = msg_json.get("conclusion", "")
            f["action_repository"] = msg_json.get("repository", "")

        # --- Public Cloud (CloudTrail / CloudWatch) ---
        elif st == "public_cloud":
            f["event_name"] = msg_json.get("event_name", "")
            f["username"] = msg_json.get("username", "")

        # --- App Container Metrics ---
        elif st == "app_container_metrics":
            f["device"] = msg_json.get("device", "")
            f["kind"] = msg_json.get("kind", "")
            f["name"] = msg_json.get("name", "")
            f["value"] = msg_json.get("value", "")

        # --- App Container Logs ---
        elif st == "app_container_log":
            f["service_name"] = msg_json.get("service_name", "")
            f["log_level"] = msg_json.get("level", "")
            f["logger_name"] = msg_json.get("logger", "")
            f["log_message"] = msg_json.get("message", "")

        # --- FastAPI Health / Heartbeat ---
        elif st == "fastapi_status":
            f["container_name"] = msg_json.get("name", "")
            f["container_status"] = msg_json.get("status", "")
            f["container_status_code"] = msg_json.get("status_code", "")
            f["container_url"] = msg_json.get("url", "")
            f["container_value"] = msg_json.get("value", "")
            f["container_message"] = msg_json.get("message", "")

        # --- Nginx Access ---
        elif st == "nginx_access":
            print(msg_json)
            f["client_ip"] = msg_json.get("remote_addr", "")
            f["request_method"] = msg_json.get("request_method", "")
            f["request_uri"] = msg_json.get("request_uri", "")
            f["response_status"] = msg_json.get("status", "")
            f["body_bytes_sent"] = msg_json.get("body_bytes_sent", "")
            f["request_time"] = msg_json.get("request_time", "")
            f["user_agent"] = msg_json.get("http_user_agent", "")

        # --- Nginx Error ---
        elif st == "nginx_error":
            msg_text = d.get("message", "")
            m_level = re.search(r"\[(\w+)\]", msg_text)
            m_detail = re.search(r": (.*)", msg_text)
            f["error_level"] = m_level.group(1) if m_level else ""
            f["error_detail"] = m_detail.group(1) if m_detail else ""

        rows.append(f)

    return pd.DataFrame(rows)


# ==========================
# 4. åŠ è½½æµ‹è¯•é›†
# ==========================
print(f"ğŸ“‚ Loading labeled test set from: {TEST_DIR}")
parquet_files = glob(os.path.join(TEST_DIR, "*.parquet"))
if not parquet_files:
    raise FileNotFoundError(f"No parquet files found in {TEST_DIR}")

df_list = [pd.read_parquet(p) for p in parquet_files]
df = pd.concat(df_list, ignore_index=True)
print(f"âœ… Test samples loaded: {len(df)}")

if "label" not in df.columns:
    raise ValueError(
        "âŒ 'label' column not found in test dataset. è¯·ç¡®è®¤ä½ å·²ç»æ‰“å¥½æ ‡ç­¾ã€‚"
    )

y_true = df["label"].astype(int).values

# ç”Ÿæˆ semantic_textï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
if "semantic_text" not in df.columns:
    print("ğŸ”§ semantic_text not found. Building from json_str ...")
    df["semantic_text"] = df["json_str"].apply(json_to_semantic)
    print("âœ… semantic_text generated.")


# ==========================
# 5. åŠ è½½ è¯­ä¹‰æ¨¡å‹
# ==========================
print("ğŸš€ Loading semantic model ...")
sem_encoder = SentenceTransformer(SEM_MODEL_NAME)
sem_scaler = joblib.load(SEM_SCALER_PATH)
sem_threshold = float(joblib.load(SEM_THRESH_PATH))

sem_input_dim = len(sem_scaler.mean_) if hasattr(sem_scaler, "mean_") else 384
sem_model = AutoEncoder(input_dim=sem_input_dim, hidden_dim=64)
sem_model.load_state_dict(torch.load(SEM_MODEL_PATH, map_location="cpu"))
sem_model.eval()
print(
    f"âœ… Semantic model loaded. input_dim={sem_input_dim}, hidden_dim=64, threshold={sem_threshold:.6f}"
)


# ==========================
# 6. åŠ è½½ ç»“æ„åŒ–æ¨¡å‹
# ==========================
print("ğŸš€ Loading structured model ...")

print(preprocessor.feature_names_in_)

str_threshold = float(joblib.load(STR_THRESH_PATH))

# ç”¨ä¸€ä¸ª dummy è¡Œæ¨æ–­ input_dim
dummy_df = pd.DataFrame([{c: "" for c in FEATURE_COLUMNS}])
dummy_X = preprocessor.transform(dummy_df)
if hasattr(dummy_X, "toarray"):
    dummy_X = dummy_X.toarray()
str_input_dim = dummy_X.shape[1]

str_model = AutoEncoder(input_dim=str_input_dim, hidden_dim=64)
str_model.load_state_dict(torch.load(STR_MODEL_PATH, map_location="cpu"))
str_model.eval()
print(
    f"âœ… Structured model loaded. input_dim={str_input_dim}, hidden_dim=64, threshold={str_threshold:.6f}"
)


# ==========================
# 7. è¯„ä¼°ï¼šè¯­ä¹‰æ¨¡å‹
# ==========================
print("ğŸ” Evaluating Semantic Model ...")
texts = df["semantic_text"].tolist()
embeddings = np.array(sem_encoder.encode(texts, batch_size=64, show_progress_bar=True))
emb_scaled = sem_scaler.transform(embeddings).astype(np.float32)

sem_tensor = torch.tensor(emb_scaled)
with torch.no_grad():
    sem_recon = sem_model(sem_tensor)
    sem_mse = ((sem_tensor - sem_recon) ** 2).mean(dim=1).cpu().numpy()

# äºŒåˆ†ç±»ï¼šå¤§äºé˜ˆå€¼ => å¼‚å¸¸(1)ï¼Œå¦åˆ™æ­£å¸¸(0)
y_pred_sem = (sem_mse > sem_threshold).astype(int)

sem_precision, sem_recall, sem_f1, _ = precision_recall_fscore_support(
    y_true, y_pred_sem, average="binary", zero_division=0
)
sem_mse_mean = float(np.mean(sem_mse))

print(
    f"âœ… Semantic Model: "
    f"Precision={sem_precision:.4f}, Recall={sem_recall:.4f}, "
    f"F1={sem_f1:.4f}, Mean MSE={sem_mse_mean:.6f}"
)


# ==========================
# 8. è¯„ä¼°ï¼šç»“æ„åŒ–æ¨¡å‹
# ==========================
print("ğŸ” Building structured features for Structured Model ...")
df_struct = build_structured_features(df)

# ä¿è¯åˆ—é½å…¨
for c in FEATURE_COLUMNS:
    if c not in df_struct.columns:
        df_struct[c] = ""

print("ğŸ” Evaluating Structured Model ...")
X_struct = preprocessor.transform(df_struct[FEATURE_COLUMNS].fillna("").astype(str))
if hasattr(X_struct, "toarray"):
    X_struct = X_struct.toarray().astype(np.float32)
else:
    X_struct = np.asarray(X_struct, dtype=np.float32)

X_tensor = torch.tensor(X_struct)
with torch.no_grad():
    recon = str_model(X_tensor)
    str_mse = ((X_tensor - recon) ** 2).mean(dim=1).cpu().numpy()

y_pred_str = (str_mse > str_threshold).astype(int)

str_precision, str_recall, str_f1, _ = precision_recall_fscore_support(
    y_true, y_pred_str, average="binary", zero_division=0
)
str_mse_mean = float(np.mean(str_mse))

print(
    f"âœ… Structured Model: "
    f"Precision={str_precision:.4f}, Recall={str_recall:.4f}, "
    f"F1={str_f1:.4f}, Mean MSE={str_mse_mean:.6f}"
)


# ==========================
# 9. ä¿å­˜æŒ‡æ ‡ + ç”»å›¾
# ==========================
metrics = {
    "model": ["semantic", "structured"],
    "precision": [sem_precision, str_precision],
    "recall": [sem_recall, str_recall],
    "f1": [sem_f1, str_f1],
    "mean_mse": [sem_mse_mean, str_mse_mean],
}
df_metrics = pd.DataFrame(metrics)
df_metrics.to_csv(CSV_PATH, index=False)
print(f"ğŸ“„ Metrics saved to: {CSV_PATH}")

# --- ç”»å¯¹æ¯”å›¾ ---
x = np.arange(4)  # 4 ä¸ªæŒ‡æ ‡
width = 0.35

semantic_vals = [sem_precision, sem_recall, sem_f1, sem_mse_mean]
structured_vals = [str_precision, str_recall, str_f1, str_mse_mean]

plt.figure(figsize=(10, 6))
plt.bar(x - width / 2, semantic_vals, width, label="Semantic")
plt.bar(x + width / 2, structured_vals, width, label="Structured")

plt.xticks(x, ["Precision", "Recall", "F1", "Mean MSE"])
plt.ylabel("Score")
plt.title("Semantic vs Structured Model Performance")
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.savefig(PLOT_PATH)

print(f"ğŸ“Š Plot saved to: {PLOT_PATH}")
print("ğŸ‰ Evaluation finished.")
