# structured_infer.py
import os
import json
import joblib
import numpy as np
import pandas as pd
import torch
import torch.nn as nn

from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import (
    col,
    get_json_object,
    regexp_extract,
    regexp_replace,
    lit,
)
from pyspark.sql.types import StringType

# ==========================================================
# ğŸ§© 0. è·¯å¾„ & Kafka é…ç½®
# ==========================================================
MODEL_DIR = "/opt/spark/work-dir/models/structured_model"
PREPROC_FILE = os.path.join(MODEL_DIR, "preprocessor.pkl")
MODEL_FILE = os.path.join(MODEL_DIR, "autoencoder.pth")
THRESH_FILE = os.path.join(MODEL_DIR, "threshold.pkl")

KAFKA_BOOTSTRAP = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"

OUT_PARQUET = "/opt/spark/work-dir/data/structured_infer"
CKPT_DIR = "/opt/spark/work-dir/data/_checkpoints_structured_infer"

# å’Œè®­ç»ƒä¿æŒå®Œå…¨ä¸€è‡´çš„ç‰¹å¾åˆ—é›†åˆï¼ˆæŒ‰ä½ è®­ç»ƒè„šæœ¬ä¸­é‚£ä»½ä¸ºå‡†ï¼‰
FEATURE_COLUMNS = [
    # â€”â€” ä¸‹é¢è¿™ç»„ç¤ºä¾‹åˆ—åè¯·ä¸ä½ è®­ç»ƒæ—¶ä½¿ç”¨çš„åˆ—ä¿æŒä¸€è‡´ â€”â€”
    # GitHub
    "commit_sha",
    "commit_author",
    "commit_date",
    "commit_message",
    "commit_url",
    "action_id",
    "action_name",
    "action_status",
    "action_conclusion",
    "action_created_at",
    "action_url",
    # Public cloud
    "event_id",
    "event_name",
    "username",
    "event_time",
    # App metrics
    "container_name",
    "cpu_perc",
    "mem_perc",
    "mem_usage",
    "net_io",
    "block_io",
    "pids",
    # App logs
    "log_time",
    "log_level",
    "logger_name",
    "log_message",
    # Nginx access
    "client_ip",
    "request_method",
    "request_uri",
    "response_status",
    "body_bytes_sent",
    "request_time",
    "user_agent",
    # Nginx error
    "error_time",
    "error_level",
    "error_detail",
    # é€šç”¨
    "source_type",  # è®­ç»ƒé‡Œè‹¥åŒ…å«å®ƒï¼Œä¸€å®šè¦ä¿ç•™ï¼›ä¸éœ€è¦å¯ç§»é™¤
]

# ==========================================================
# ğŸ§© 1. åŠ è½½æ¨¡å‹ç»„ä»¶
# ==========================================================
print(f"ğŸš€ Loading components from {MODEL_DIR} ...")
preprocessor = joblib.load(PREPROC_FILE)  # sklearn ColumnTransformer(Pipeline)
threshold = joblib.load(THRESH_FILE) if os.path.exists(THRESH_FILE) else 0.01


class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.0)
        )
        self.decoder = nn.Sequential(nn.Linear(hidden_dim, input_dim))

    def forward(self, x):
        z = self.encoder(x)
        return self.decoder(z)


# ç”¨ä¸€ä¸ªå‡çš„ 1 è¡Œè¾“å…¥æ¢æµ‹ input_dim
_probe = preprocessor.transform(pd.DataFrame([{c: None for c in FEATURE_COLUMNS}]))

input_dim = _probe.shape[1]

model = AutoEncoder(input_dim=input_dim, hidden_dim=64)
model.load_state_dict(torch.load(MODEL_FILE, map_location="cpu"))
model.eval()
print(f"âœ… Model loaded. input_dim={input_dim}, threshold={threshold:.6f}")

# ==========================================================
# ğŸ§© 2. Spark åˆå§‹åŒ– & Kafka æº
# ==========================================================
spark = (
    SparkSession.builder.appName("Structured_Streaming_Inference")
    .config("spark.sql.shuffle.partitions", "4")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)
df = df_kafka.selectExpr("CAST(value AS STRING) AS json_str")

# ==========================================================
# ğŸ§© 3. é€šç”¨å­—æ®µ
# ==========================================================
df_base = df.select(
    get_json_object(col("json_str"), "$.source_type").alias("source_type"),
    get_json_object(col("json_str"), "$.timestamp").alias("timestamp"),
    get_json_object(col("json_str"), "$.message").alias("message"),
)

# ==========================================================
# ğŸ§© 4. å„æºè§£æï¼ˆä¸ structure_preprocessing.py ä¸€è‡´ï¼‰
# ==========================================================

# GitHub Commits
df_github_commits = df_base.filter(col("source_type") == "github_commits").select(
    col("source_type"),
    get_json_object(col("message"), "$.sha").alias("commit_sha"),
    get_json_object(col("message"), "$.author").alias("commit_author"),
    get_json_object(col("message"), "$.date").alias("commit_date"),
    get_json_object(col("message"), "$.message").alias("commit_message"),
    get_json_object(col("message"), "$.url").alias("commit_url"),
)

# GitHub Actions
df_github_actions = df_base.filter(col("source_type") == "github_actions").select(
    col("source_type"),
    get_json_object(col("message"), "$.id").alias("action_id"),
    get_json_object(col("message"), "$.name").alias("action_name"),
    get_json_object(col("message"), "$.status").alias("action_status"),
    get_json_object(col("message"), "$.conclusion").alias("action_conclusion"),
    get_json_object(col("message"), "$.created_at").alias("action_created_at"),
    get_json_object(col("message"), "$.url").alias("action_url"),
)

# Public cloud (CloudTrail/CloudWatch)
df_public_cloud = df_base.filter(col("source_type") == "public_cloud").select(
    col("source_type"),
    get_json_object(col("message"), "$.event_id").alias("event_id"),
    get_json_object(col("message"), "$.event_name").alias("event_name"),
    get_json_object(col("message"), "$.username").alias("username"),
    get_json_object(col("message"), "$.event_time").alias("event_time"),
)

# App container metrics
df_app_metrics = df_base.filter(col("source_type") == "app_container_metrics").select(
    col("source_type"),
    get_json_object(col("message"), "$.Container").alias("container_name"),
    get_json_object(col("message"), "$.CPUPerc").alias("cpu_perc"),
    get_json_object(col("message"), "$.MemPerc").alias("mem_perc"),
    get_json_object(col("message"), "$.MemUsage").alias("mem_usage"),
    get_json_object(col("message"), "$.NetIO").alias("net_io"),
    get_json_object(col("message"), "$.BlockIO").alias("block_io"),
    get_json_object(col("message"), "$.PIDs").alias("pids"),
)

# App container logs (JSON åŒ–)
df_app_logs = df_base.filter(col("source_type") == "app_container_log").select(
    col("source_type"),
    get_json_object(col("message"), "$.time").alias("log_time"),
    get_json_object(col("message"), "$.level").alias("log_level"),
    get_json_object(col("message"), "$.logger").alias("logger_name"),
    get_json_object(col("message"), "$.message").alias("log_message"),
)

# Nginx access (JSON æ ¼å¼)
df_nginx = df_base.filter(col("source_type") == "nginx").select(
    col("source_type"),
    get_json_object(col("message"), "$.remote_addr").alias("client_ip"),
    get_json_object(col("message"), "$.request_method").alias("request_method"),
    get_json_object(col("message"), "$.request_uri").alias("request_uri"),
    get_json_object(col("message"), "$.status").alias("response_status"),
    get_json_object(col("message"), "$.body_bytes_sent").alias("body_bytes_sent"),
    get_json_object(col("message"), "$.request_time").alias("request_time"),
    get_json_object(col("message"), "$.http_user_agent").alias("user_agent"),
)

# Nginx errorï¼ˆå¤šæ ·å¼ç®€åŒ–ï¼‰
df_nginx_error = df_base.filter(col("source_type") == "nginx_error").select(
    col("source_type"),
    regexp_extract("message", r"^(\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2})", 1).alias(
        "error_time"
    ),
    regexp_extract("message", r"\[(\w+)\]", 1).alias("error_level"),
    regexp_extract("message", r": (.*)", 1).alias("error_detail"),
)

# åˆå¹¶ç»Ÿä¸€è¡¨
df_final = (
    df_github_commits.unionByName(df_github_actions, allowMissingColumns=True)
    .unionByName(df_public_cloud, allowMissingColumns=True)
    .unionByName(df_app_metrics, allowMissingColumns=True)
    .unionByName(df_app_logs, allowMissingColumns=True)
    .unionByName(df_nginx, allowMissingColumns=True)
    .unionByName(df_nginx_error, allowMissingColumns=True)
)

# ä¸ºç¼ºå¤±åˆ—è¡¥ç©ºï¼Œç¡®ä¿åˆ—å¯¹é½
for c in FEATURE_COLUMNS:
    if c not in df_final.columns:
        df_final = df_final.withColumn(c, lit(None).cast(StringType()))

# åªä¿ç•™è®­ç»ƒéœ€è¦çš„åˆ— + ä¾¿äºæº¯æºçš„å­—æ®µ
df_features = df_final.select(
    *FEATURE_COLUMNS, col("source_type").alias("src_type_runtime")
)


# ==========================================================
# ğŸ§© 5. foreachBatch æ¨ç†
# ==========================================================
def infer_batch(batch_df, epoch_id: int):
    pd_df = batch_df.select(*FEATURE_COLUMNS, "src_type_runtime").toPandas()
    if pd_df.empty:
        return

    # 1) é¢„å¤„ç†ï¼ˆOneHot + æ ‡å‡†åŒ–ï¼‰
    X = preprocessor.transform(pd_df[FEATURE_COLUMNS])  # ndarray / æˆ–è€…ç¨€ç–
    X_dense = X.toarray() if hasattr(X, "toarray") else np.asarray(X, dtype=np.float32)

    # 2) æ¨ç†ï¼ˆAutoEncoder é‡æ„è¯¯å·®ï¼‰
    X_tensor = torch.tensor(X_dense, dtype=torch.float32)
    with torch.no_grad():
        recon = model(X_tensor)
        mse = ((X_tensor - recon) ** 2).mean(dim=1).cpu().numpy()

    # 3) æ‰“æ ‡ç­¾
    labels = np.where(
        mse > threshold * 1.5,
        "high_anomaly",
        np.where(mse > threshold, "low_anomaly", "normal"),
    )

    # 4) è¾“å‡ºæ‹¼æ¥
    out_pdf = pd_df.copy()
    out_pdf["reconstruction_error"] = mse.round(6)
    out_pdf["threshold"] = float(threshold)
    out_pdf["prediction"] = labels

    # æ‰“å°éƒ¨åˆ†ç»“æœï¼ˆconsole è§‚å¯Ÿï¼‰
    print(f"\n=== Inference Epoch {epoch_id} | rows={len(out_pdf)} ===")
    print(
        out_pdf[["src_type_runtime", "prediction", "reconstruction_error"]]
        .head(10)
        .to_string(index=False)
    )
    out_pdf = out_pdf.astype(str)

    # å†™ Parquetï¼ˆAppendï¼‰
    spark.createDataFrame(out_pdf).write.mode("append").parquet(OUT_PARQUET)


# ==========================================================
# ğŸ§© 6. å¯åŠ¨æµ
# ==========================================================
query = (
    df_features.writeStream.outputMode("append")
    .foreachBatch(infer_batch)
    .option("checkpointLocation", CKPT_DIR)
    .start()
)

query.awaitTermination()
