"""
Semantic Vector Streaming Pipeline (with Console Output)
========================================================
âœ… ä» Kafka å®æ—¶è¯»å–å¤šæºç›‘æ§æ•°æ®
âœ… è§£æ JSON â†’ æ‹¼æ¥è¯­ä¹‰å¥ â†’ SentenceTransformer å‘é‡åŒ–
âœ… è½åœ° Parquet + å®æ—¶æ‰“å°åˆ°æ§åˆ¶å°
========================================================
"""

import os
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, get_json_object
from pyspark.sql.types import StringType, ArrayType, FloatType
from sentence_transformers import SentenceTransformer

# ==========================================================
# ğŸ§© Step 0. é…ç½®
# ==========================================================
KAFKA_SERVERS = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"
OUTPUT_PATH = "/opt/spark/work-dir/data/semantic_vectors"
CHECKPOINT_PATH = "/opt/spark/work-dir/data/_checkpoints_semantic_vectors"

MODEL_NAME = "all-MiniLM-L6-v2"

print(f"ğŸš€ Initializing SentenceTransformer: {MODEL_NAME}")
model = SentenceTransformer(MODEL_NAME)

# ==========================================================
# ğŸ§© Step 1. åˆå§‹åŒ– Spark
# ==========================================================
spark = (
    SparkSession.builder.appName("SemanticVectorStreaming")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .config("spark.sql.execution.arrow.pyspark.enabled", True)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark initialized successfully.")

# ==========================================================
# ğŸ§© Step 2. Kafka Source
# ==========================================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_SERVERS)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

df_raw = df_kafka.selectExpr("CAST(value AS STRING) as message")


# ==========================================================
# ğŸ§© Step 3. JSON â†’ è¯­ä¹‰å¥
# ==========================================================
def json_to_semantic(text):
    try:
        data = json.loads(text)
        parts = []
        for k, v in data.items():
            if isinstance(v, dict):
                for kk, vv in v.items():
                    parts.append(f"{k}.{kk}: {vv}")
            else:
                parts.append(f"{k}: {v}")
        return ". ".join(parts)
    except Exception:
        return "[INVALID_JSON]"


semantic_udf = udf(json_to_semantic, StringType())
df_semantic = df_raw.withColumn("semantic_text", semantic_udf(col("message")))

df_semantic = df_semantic.withColumn(
    "source_type", get_json_object(col("message"), "$.source_type")
)
df_semantic = df_semantic.withColumn(
    "ingest_time", get_json_object(col("message"), "$.timestamp")
)


# ==========================================================
# ğŸ§© Step 4. SentenceTransformer å‘é‡åŒ–
# ==========================================================
def encode_text(text):
    try:
        embedding = model.encode(text)
        return [float(x) for x in embedding]
    except Exception:
        return []


encode_udf = udf(encode_text, ArrayType(FloatType()))

df_vec = df_semantic.withColumn("embedding", encode_udf(col("semantic_text")))

# ==========================================================
# ğŸ§© Step 5. è¾“å‡ºåˆ° Parquet + Console
# ==========================================================
# --- å®æ—¶æ‰“å°åˆ°æ§åˆ¶å° ---
query_console = (
    df_vec.select("source_type")
    .writeStream.outputMode("append")
    .format("console")
    .option("truncate", False)
    .option("numRows", 5)
    .start()
)

# --- å†™å…¥ Parquet ---
query_parquet = (
    df_vec.select("source_type", "ingest_time", "semantic_text", "embedding")
    .writeStream.outputMode("append")
    .format("parquet")
    .option("path", OUTPUT_PATH)
    .option("checkpointLocation", CHECKPOINT_PATH)
    .trigger(processingTime="60 seconds")
    .start()
)

spark.streams.awaitAnyTermination()
