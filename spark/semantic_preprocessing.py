"""
Semantic Vector Streaming Pipeline (Simplified Version)
========================================================
âœ… ä»…æå– Kafka æ¶ˆæ¯ä¸­çš„ message å­—æ®µ
âœ… å»é™¤æ‰€æœ‰ Vector å…ƒä¿¡æ¯ï¼ˆfileã€hostã€timestamp ç­‰ï¼‰
âœ… è¿‡æ»¤æ—¶é—´ç±»å­—æ®µ (time/timestamp/date)
âœ… SentenceTransformer å‘é‡åŒ– â†’ Parquet + æ§åˆ¶å°è¾“å‡º
========================================================
"""

import os
import json
import time
import threading
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, get_json_object
from pyspark.sql.types import StringType, ArrayType, FloatType
from sentence_transformers import SentenceTransformer

# ==========================================================
# ğŸ§© Step 0. é…ç½®
# ==========================================================
KAFKA_SERVERS = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"
OUTPUT_PATH = "/opt/spark/work-dir/data/semantic_vectors"
CHECKPOINT_PATH = "/opt/spark/work-dir/data/_checkpoints_semantic_vectors"
MODEL_NAME = "all-MiniLM-L12-v2"
TARGET_ROWS = 5000

print(f"ğŸš€ Initializing SentenceTransformer: {MODEL_NAME}")
model = SentenceTransformer(MODEL_NAME)

# ==========================================================
# ğŸ§© Step 1. åˆå§‹åŒ– Spark
# ==========================================================
spark = (
    SparkSession.builder.appName("SemanticVectorStreaming")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .config("spark.sql.execution.arrow.pyspark.enabled", True)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark initialized successfully.")

# ==========================================================
# ğŸ§© Step 2. Kafka Source
# ==========================================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_SERVERS)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

df_raw = df_kafka.selectExpr("CAST(value AS STRING) as message")


# ==========================================================
# ğŸ§© Step 3. JSON â†’ Semantic Text
# ==========================================================
def json_to_semantic(text):
    try:
        data = json.loads(text)
        if isinstance(data, dict) and "message" in data:
            try:
                msg = json.loads(data["message"])
            except Exception:
                msg = data["message"]
        else:
            msg = data

        parts = []
        if isinstance(msg, dict):
            for k, v in msg.items():
                if any(
                    t in k.lower() for t in ["time", "timestamp", "date", "created_at"]
                ):
                    continue
                parts.append(f"{k}={v}")
        else:
            parts.append(str(msg))

        return " ".join(parts)
    except Exception:
        return "[INVALID_JSON]"


semantic_udf = udf(json_to_semantic, StringType())
df_semantic = df_raw.withColumn("semantic_text", semantic_udf(col("message")))

df_semantic = df_semantic.withColumn(
    "source_type", get_json_object(col("message"), "$.source_type")
)
df_semantic = df_semantic.withColumn(
    "ingest_time", get_json_object(col("message"), "$.timestamp")
)


# ==========================================================
# ğŸ§© Step 4. SentenceTransformer å‘é‡åŒ–
# ==========================================================
def encode_text(text):
    try:
        embedding = model.encode(text)
        return [float(x) for x in embedding]
    except Exception:
        return []


encode_udf = udf(encode_text, ArrayType(FloatType()))
df_vec = df_semantic.withColumn("embedding", encode_udf(col("semantic_text")))


# ==========================================================
# ğŸ§© Step 5. è‡ªåŠ¨é€€å‡ºä¸è¾“å‡º (æ ¸å¿ƒä¿®æ”¹)
# ==========================================================

global_counter = {"total_rows": 0}


# --- 1. foreachBatch å¤„ç†å‡½æ•° ---
def write_and_count(batch_df, batch_id):
    """
    å¤„ç†æ¯ä¸ªå¾®æ‰¹æ¬¡ï¼šå†™å…¥ Parquetï¼Œå¹¶æ›´æ–°å…¨å±€è®¡æ•°å™¨ã€‚
    """
    global global_counter

    # å†™å…¥ Parquet (å–ä»£åŸå§‹çš„å†™å…¥é€»è¾‘)
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ batch_df.writeï¼Œè€Œä¸æ˜¯ writeStreamã€‚
    batch_df.select(
        "source_type", "ingest_time", "semantic_text", "embedding"
    ).write.mode("append").format("parquet").save(OUTPUT_PATH)

    # æ›´æ–°å…¨å±€è®¡æ•°å™¨
    count = batch_df.count()
    global_counter["total_rows"] += count

    # æ‰“å°è¿›åº¦
    print(
        f"| Batch {batch_id}: Processed {count} rows. Total: {global_counter['total_rows']}/{TARGET_ROWS} |"
    )


# --- 2. å¯åŠ¨æµå¼æŸ¥è¯¢ ---

# ä½¿ç”¨ foreachBatch æ›¿ä»£ä¹‹å‰çš„å†™å…¥ Parquet
query_parquet = (
    df_vec.writeStream.outputMode("append")
    .option("checkpointLocation", CHECKPOINT_PATH)
    .foreachBatch(write_and_count)  # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
    .start()
)

# --- 3. ä¸»çº¿ç¨‹ç›‘æ§ä¸ç»ˆæ­¢ ---

print(f"\nâ° Streaming query started. Monitoring total rows. Target: {TARGET_ROWS}...")

try:
    # å¾ªç¯ç›‘æ§è®¡æ•°å™¨ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡è¡Œæ•°
    while global_counter["total_rows"] < TARGET_ROWS and query_parquet.isActive:
        time.sleep(10)  # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡è®¡æ•°å™¨

    if query_parquet.isActive:
        print(f"\nğŸ›‘ Target row count ({TARGET_ROWS}) reached. Stopping query...")
        query_parquet.stop()

except Exception as e:
    print(f"\nâš ï¸ Error occurred or interruption: {e}. Stopping query...")
    if query_parquet.isActive:
        query_parquet.stop()

spark.stop()
print("âœ… Spark session terminated.")
