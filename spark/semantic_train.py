"""
Train AutoEncoder from Parquet Embeddings
=========================================
âœ… è‡ªåŠ¨éå†æ‰€æœ‰ parquet æ–‡ä»¶ï¼ˆä¸å†é™åˆ¶ 10 ä¸ªï¼‰
âœ… å¿½ç•¥æŸåæ–‡ä»¶å¹¶æ±‡æ€»çœŸå®è®°å½•æ¡æ•°
âœ… è®­ç»ƒ PyTorch AutoEncoderï¼ˆé‡æ„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼‰
âœ… ä¿å­˜ scalerã€æ¨¡å‹æƒé‡ã€é˜ˆå€¼
=========================================
"""

import os
import glob
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm


# ==========================================================
# ğŸ§© AutoEncoder å®šä¹‰
# ==========================================================
class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Dropout(0.2)
        )
        self.decoder = nn.Sequential(nn.Linear(hidden_dim, input_dim))

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


# ==========================================================
# ğŸ§  è®­ç»ƒé€»è¾‘å°è£…
# ==========================================================
def train_autoencoder(
    parquet_dir: str = "/opt/spark/work-dir/data/semantic_vectors",
    model_dir: str = "/opt/spark/work-dir/models/prediction_model",
    epochs: int = 10,
    batch_size: int = 256,
    lr: float = 1e-3,
    hidden_dim: int = 64,
):
    os.makedirs(model_dir, exist_ok=True)

    # Step 1. åŠ è½½æ‰€æœ‰ Parquet æ–‡ä»¶
    print(f"ğŸ“‚ Loading parquet files from: {parquet_dir}")
    files = [
        f
        for f in glob.glob(os.path.join(parquet_dir, "*.parquet"))
        if os.path.getsize(f) > 0
    ]

    print(f"ğŸ“ Found {len(files)} parquet files.")
    if not files:
        raise RuntimeError("âŒ No valid parquet files found!")

    dfs = []
    for f in tqdm(files, desc="ğŸ“¥ Reading parquet files"):
        try:
            df_part = pd.read_parquet(f)
            if "embedding" in df_part.columns:
                valid_df = df_part[["embedding"]].dropna()
                if len(valid_df) > 0:
                    dfs.append(valid_df)
        except Exception as e:
            print(f"âš ï¸ Skip {f}: {e}")

    if not dfs:
        raise RuntimeError("âŒ No embeddings found in parquet data.")

    df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… Loaded {len(df):,} embeddings from {len(dfs)} valid parquet files.")
    X = np.stack(df["embedding"].to_numpy())
    print(f"ğŸ§  Embedding shape: {X.shape}")

    # Step 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, os.path.join(model_dir, "scaler.pkl"))
    print("ğŸ’¾ Saved scaler.pkl")

    # Step 3. åˆå§‹åŒ–æ¨¡å‹
    input_dim = X_scaled.shape[1]
    model = AutoEncoder(input_dim=input_dim, hidden_dim=hidden_dim)
    print(f"ğŸ§© AutoEncoder initialized: input_dim={input_dim}, hidden_dim={hidden_dim}")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)
    dataset = torch.utils.data.TensorDataset(X_tensor)
    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Step 4. è®­ç»ƒ
    print(f"ğŸš€ Starting training for {epochs} epochs...")
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for (batch,) in loader:
            optimizer.zero_grad()
            output = model(batch)
            loss = criterion(output, batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss/len(loader):.6f}")

    # Step 5. è®¡ç®—é˜ˆå€¼ï¼ˆ95%åˆ†ä½ï¼‰
    model.eval()
    with torch.no_grad():
        reconstructed = model(X_tensor)
        mse = torch.mean((X_tensor - reconstructed) ** 2, dim=1).cpu().numpy()

    threshold = float(np.percentile(mse, 97.5))
    print(f"ğŸ“Š Computed 97.5th percentile threshold: {threshold:.6f}")
    joblib.dump(threshold, os.path.join(model_dir, "threshold.pkl"))

    # Step 6. ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), os.path.join(model_dir, "autoencoder.pth"))
    print(f"ğŸ’¾ Saved model to {model_dir}/autoencoder.pth")

    print("\nâœ… Training complete.")
    print(f"ğŸ“ Model directory: {model_dir}")


# ==========================================================
# âœ… Main å…¥å£
# ==========================================================
if __name__ == "__main__":
    train_autoencoder()
