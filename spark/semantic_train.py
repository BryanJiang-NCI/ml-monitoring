"""
Train AutoEncoder from Parquet Embeddings
=========================================
âœ… ä»è¯­ä¹‰å‘é‡ parquet æ–‡ä»¶ä¸­è¯»å– embedding
âœ… è®­ç»ƒ PyTorch AutoEncoderï¼ˆé‡æ„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼‰
âœ… ä¿å­˜ scalerã€æ¨¡å‹æƒé‡ã€é˜ˆå€¼
=========================================
"""

import os
import glob
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm


# ==========================================================
# ğŸ§© AutoEncoder å®šä¹‰ï¼ˆå¯ä¾› import ä½¿ç”¨ï¼‰
# ==========================================================
class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(AutoEncoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


# ==========================================================
# ğŸ§  è®­ç»ƒé€»è¾‘å°è£…ä¸ºå‡½æ•°
# ==========================================================
def train_autoencoder(
    parquet_dir: str = "/opt/spark/work-dir/data/semantic_vectors",
    model_dir: str = "/opt/spark/work-dir/models/prediction_model",
    epochs: int = 10,
    batch_size: int = 256,
    lr: float = 1e-3,
):
    os.makedirs(model_dir, exist_ok=True)

    # Step 1. åŠ è½½ Parquet æ–‡ä»¶
    print(f"ğŸ“‚ Loading parquet files from: {parquet_dir}")
    files = [
        f
        for f in glob.glob(os.path.join(parquet_dir, "*.parquet"))
        if os.path.getsize(f) > 0
    ]

    if not files:
        raise RuntimeError("âŒ No valid parquet files found!")

    dfs = []
    for f in tqdm(files[:10], desc="ğŸ“¥ Loading parquet (limit 10 files for now)"):
        try:
            df_part = pd.read_parquet(f)
            if "embedding" in df_part.columns:
                dfs.append(df_part[["embedding"]].dropna())
        except Exception as e:
            print(f"âš ï¸ Skip {f}: {e}")

    if not dfs:
        raise RuntimeError("âŒ No embeddings found in parquet data.")

    df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… Loaded {len(df):,} embeddings.")

    X = np.stack(df["embedding"].to_numpy())
    print(f"ğŸ§  Embedding shape: {X.shape}")

    # Step 2. æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, os.path.join(model_dir, "scaler.pkl"))
    print("ğŸ’¾ Saved scaler.pkl")

    # Step 3. åˆå§‹åŒ–æ¨¡å‹
    input_dim = X_scaled.shape[1]
    model = AutoEncoder(input_dim=input_dim, hidden_dim=128)
    print(f"ğŸ§© AutoEncoder initialized: input_dim={input_dim}, hidden_dim=128")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)
    dataset = torch.utils.data.TensorDataset(X_tensor)
    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    # Step 4. è®­ç»ƒ
    print(f"ğŸš€ Starting training for {epochs} epochs...")
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for (batch,) in loader:
            optimizer.zero_grad()
            output = model(batch)
            loss = criterion(output, batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss/len(loader):.6f}")

    # Step 5. è®¡ç®—é˜ˆå€¼ï¼ˆ95%åˆ†ä½ï¼‰
    model.eval()
    with torch.no_grad():
        reconstructed = model(X_tensor)
        mse = torch.mean((X_tensor - reconstructed) ** 2, dim=1).cpu().numpy()

    threshold = float(np.percentile(mse, 95))
    joblib.dump(threshold, os.path.join(model_dir, "threshold.pkl"))
    print(f"ğŸ“Š Computed 95th percentile threshold: {threshold:.6f}")

    # Step 6. ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), os.path.join(model_dir, "autoencoder.pth"))
    print(f"ğŸ’¾ Saved model to {model_dir}/autoencoder.pth")

    print("\nâœ… Training complete.")
    print(f"ğŸ“ Model directory: {model_dir}")
    print(f"ğŸ§© Threshold (95% MSE): {threshold:.6f}")


# ==========================================================
# âœ… ä»…åœ¨ç›´æ¥è¿è¡Œæ—¶æ‰§è¡Œè®­ç»ƒé€»è¾‘
# ==========================================================
if __name__ == "__main__":
    train_autoencoder()
