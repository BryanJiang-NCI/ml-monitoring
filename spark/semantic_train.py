"""
Train AutoEncoder from Parquet Embeddings
=========================================
âœ… ä»è¯­ä¹‰å‘é‡ parquet æ–‡ä»¶ä¸­è¯»å– embedding
âœ… è®­ç»ƒ PyTorch AutoEncoderï¼ˆé‡æ„å¼‚å¸¸æ£€æµ‹æ¨¡å‹ï¼‰
âœ… ä¿å­˜ scalerã€æ¨¡å‹æƒé‡ã€é˜ˆå€¼
=========================================
"""

import os
import glob
import joblib
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

# ==========================================================
# ğŸ§© Step 0. é…ç½®è·¯å¾„
# ==========================================================
PARQUET_DIR = "./spark-data/semantic_vectors"
MODEL_DIR = "./spark-apps/models/autoencoder_tfidf_torch"
os.makedirs(MODEL_DIR, exist_ok=True)

# ==========================================================
# ğŸ§© Step 1. åŠ è½½ Parquet æ•°æ®
# ==========================================================
print(f"ğŸ“‚ Loading parquet files from: {PARQUET_DIR}")
files = [
    f
    for f in glob.glob(os.path.join(PARQUET_DIR, "*.parquet"))
    if os.path.getsize(f) > 0
]

if not files:
    raise RuntimeError("âŒ No valid parquet files found!")

dfs = []
for f in tqdm(files[:10], desc="ğŸ“¥ Loading parquet (limit 10 files for now)"):
    try:
        df_part = pd.read_parquet(f)
        if "embedding" in df_part.columns:
            dfs.append(df_part[["embedding"]].dropna())
    except Exception as e:
        print(f"âš ï¸ Skip {f}: {e}")

if not dfs:
    raise RuntimeError("âŒ No embeddings found in parquet data.")

df = pd.concat(dfs, ignore_index=True)
print(f"âœ… Loaded {len(df):,} embeddings.")

# è½¬ä¸º numpy
X = np.stack(df["embedding"].to_numpy())
print(f"ğŸ§  Embedding shape: {X.shape}")

# ==========================================================
# ğŸ§© Step 2. æ ‡å‡†åŒ–
# ==========================================================
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
joblib.dump(scaler, os.path.join(MODEL_DIR, "scaler.pkl"))
print("ğŸ’¾ Saved scaler.pkl")


# ==========================================================
# ğŸ§© Step 3. å®šä¹‰ AutoEncoder
# ==========================================================
class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(AutoEncoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


input_dim = X_scaled.shape[1]
model = AutoEncoder(input_dim=input_dim, hidden_dim=128)
print(f"ğŸ§© AutoEncoder initialized: input_dim={input_dim}, hidden_dim=128")

# ==========================================================
# ğŸ§© Step 4. è®­ç»ƒ
# ==========================================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)
dataset = torch.utils.data.TensorDataset(X_tensor)
loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)

criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)

EPOCHS = 10
print(f"ğŸš€ Starting training for {EPOCHS} epochs...")
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for (batch,) in loader:
        optimizer.zero_grad()
        output = model(batch)
        loss = criterion(output, batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch [{epoch+1}/{EPOCHS}] - Loss: {total_loss/len(loader):.6f}")

# ==========================================================
# ğŸ§© Step 5. è®¡ç®—é˜ˆå€¼ï¼ˆ95åˆ†ä½ï¼‰
# ==========================================================
model.eval()
with torch.no_grad():
    reconstructed = model(X_tensor)
    mse = torch.mean((X_tensor - reconstructed) ** 2, dim=1).cpu().numpy()

threshold = float(np.percentile(mse, 95))
joblib.dump(threshold, os.path.join(MODEL_DIR, "threshold.pkl"))
print(f"ğŸ“Š Computed 95th percentile threshold: {threshold:.6f}")

# ==========================================================
# ğŸ§© Step 6. ä¿å­˜æ¨¡å‹
# ==========================================================
torch.save(model.state_dict(), os.path.join(MODEL_DIR, "autoencoder.pth"))
print(f"ğŸ’¾ Saved model to {MODEL_DIR}/autoencoder.pth")

print("\nâœ… Training complete.")
print(f"ğŸ“ Model directory: {MODEL_DIR}")
print(f"ğŸ§© Threshold (95% MSE): {threshold:.6f}")
