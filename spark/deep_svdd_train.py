"""
Deep SVDD Training Script
=========================
âœ… ä» Parquet å‘é‡æ–‡ä»¶åŠ è½½ embedding æ•°æ®
âœ… ä»…ä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒï¼ˆå‡è®¾æ•°æ®é›†å¤§éƒ¨åˆ†ä¸ºæ­£å¸¸ï¼‰
âœ… è®­ç»ƒä¸€ä¸ªç‰¹å¾æå–ç½‘ç»œå¹¶è®¡ç®—ä¸­å¿ƒç‚¹ c
âœ… ä¿å­˜æ¨¡å‹ä¸é˜ˆå€¼æ–‡ä»¶ï¼ˆthreshold.pkl, model.pth, center.pklï¼‰
=========================
"""

import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
import joblib
import numpy as np
import pandas as pd
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler


# -----------------------
# æ¨¡å‹å®šä¹‰
# -----------------------
class DeepSVDD(nn.Module):
    def __init__(self, input_dim=384, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 32),
        )

    def forward(self, x):
        return self.net(x)


# -----------------------
# è®­ç»ƒä¸»é€»è¾‘
# -----------------------
def train_deep_svdd(
    parquet_dir="/opt/spark/work-dir/data/semantic_vectors",
    model_dir="/opt/spark/work-dir/models/deep_svdd_model",
    epochs=20,
    lr=1e-3,
    batch_size=128,
):
    os.makedirs(model_dir, exist_ok=True)

    # Step 1: è¯»å–æ‰€æœ‰ parquet å‘é‡æ–‡ä»¶
    files = glob.glob(os.path.join(parquet_dir, "*.parquet"))
    dfs = []
    for f in tqdm(files, desc="ğŸ“¥ Reading parquet files"):
        try:
            df = pd.read_parquet(f)
            if "embedding" in df.columns:
                dfs.append(df[["embedding"]].dropna())
        except Exception as e:
            print("âš ï¸ Skip:", f, e)

    if not dfs:
        raise RuntimeError("âŒ No embeddings found in parquet data.")

    df = pd.concat(dfs, ignore_index=True)
    X = np.stack(df["embedding"].to_numpy())
    print(f"âœ… Loaded {len(X)} samples with shape {X.shape}")

    # Step 2: æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    joblib.dump(scaler, os.path.join(model_dir, "scaler.pkl"))
    print("ğŸ’¾ Saved scaler.pkl")

    # Step 3: æ¨¡å‹åˆå§‹åŒ–
    input_dim = X_scaled.shape[1]
    model = DeepSVDD(input_dim=input_dim)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)

    X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)
    dataset = torch.utils.data.TensorDataset(X_tensor)
    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Step 4: è®¡ç®—åˆå§‹ä¸­å¿ƒç‚¹ c
    model.eval()
    with torch.no_grad():
        c = torch.mean(model(X_tensor), dim=0)
    torch.save(c, os.path.join(model_dir, "center.pt"))
    print("ğŸ“ Initial center computed and saved.")

    # Step 5: è®­ç»ƒ
    optimizer = optim.Adam(model.parameters(), lr=lr)
    for epoch in range(epochs):
        total_loss = 0
        model.train()
        for (batch,) in loader:
            optimizer.zero_grad()
            outputs = model(batch)
            loss = torch.mean((outputs - c) ** 2)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch [{epoch+1}/{epochs}] - Loss: {total_loss/len(loader):.6f}")

    # Step 6: è®¡ç®—é˜ˆå€¼
    model.eval()
    with torch.no_grad():
        dist = torch.mean((model(X_tensor) - c) ** 2, dim=1).cpu().numpy()
    threshold = np.percentile(dist, 97.5)
    mean_dist = np.mean(dist)
    print(f"ğŸ“Š Computed 97.5th percentile threshold: {threshold:.6f}")
    print(f"ğŸ“ˆ Mean distance: {mean_dist:.6f}")

    joblib.dump(threshold, os.path.join(model_dir, "threshold.pkl"))
    torch.save(model.state_dict(), os.path.join(model_dir, "deep_svdd.pth"))
    print(f"ğŸ’¾ Model saved to {model_dir}")
    print("âœ… Training complete.")


if __name__ == "__main__":
    train_deep_svdd()
