"""
Structured Data AutoEncoder Training
====================================
âœ… ä» structured_data Parquet è¯»å–ç»“æ„åŒ–æ—¥å¿—æ•°æ®
âœ… å¯¹åˆ†ç±»å­—æ®µåš OneHot ç¼–ç 
âœ… å¯¹æ•°å€¼å­—æ®µåšæ ‡å‡†åŒ–
âœ… ä½¿ç”¨ AutoEncoder è¿›è¡Œæ— ç›‘ç£é‡æ„è®­ç»ƒ
âœ… è¾“å‡ºæ¨¡å‹æ–‡ä»¶ + é˜ˆå€¼
====================================
"""

import os
import torch
import torch.nn as nn
import joblib
import numpy as np
import pandas as pd
from pyspark.sql import SparkSession
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split

# ==========================================================
# ğŸ§© Step 0. è·¯å¾„é…ç½®
# ==========================================================
DATA_PATH = "/opt/spark/work-dir/data/structured_data"
MODEL_DIR = "/opt/spark/work-dir/models/structured_model"
os.makedirs(MODEL_DIR, exist_ok=True)

# ==========================================================
# ğŸ§© Step 1. Spark åˆå§‹åŒ–å¹¶è¯»å– Parquet
# ==========================================================
spark = SparkSession.builder.appName("Structured_AutoEncoder_Train").getOrCreate()
spark.sparkContext.setLogLevel("WARN")

print(f"ğŸ“¦ Loading structured parquet data from {DATA_PATH} ...")
df = spark.read.parquet(DATA_PATH)
pdf = df.toPandas()
print(f"âœ… Loaded {len(pdf)} records, {len(pdf.columns)} columns.")

# ==========================================================
# ğŸ§© Step 2. å­—æ®µé€‰æ‹©ä¸ç±»å‹æ¨æ–­
# ==========================================================
# åªä¿ç•™å‰å‡ ä¸ªå…³é”®å­—æ®µï¼ˆå¤ªå¤šå­—æ®µæ¨¡å‹ä¼šå¤ªç¨€ç–ï¼‰
cols_keep = [
    "source_type",
    "commit_author",
    "commit_message",
    "action_status",
    "action_conclusion",
    "event_name",
    "username",
    "cpu_perc",
    "mem_perc",
    "container_name",
    "log_level",
    "log_message",
    "response_status",
    "error_level",
]

pdf = pdf[cols_keep].fillna("")

# å°è¯•æ¨æ–­å“ªäº›å­—æ®µæ˜¯æ•°å€¼åˆ—
numeric_cols = []
categorical_cols = []

for c in pdf.columns:
    try:
        # å°è¯•æŠŠç™¾åˆ†æ¯”æˆ–æ•°å€¼åˆ—è½¬ float
        if pdf[c].astype(str).str.contains("%").any():
            pdf[c] = pdf[c].astype(str).str.replace("%", "").astype(float)
            numeric_cols.append(c)
        elif pd.to_numeric(pdf[c], errors="coerce").notnull().mean() > 0.9:
            pdf[c] = pd.to_numeric(pdf[c], errors="coerce").fillna(0)
            numeric_cols.append(c)
        else:
            categorical_cols.append(c)
    except Exception:
        categorical_cols.append(c)

print(f"ğŸ§© Numeric columns: {numeric_cols}")
print(f"ğŸ§© Categorical columns: {categorical_cols}")

# ==========================================================
# ğŸ§© Step 3. ç‰¹å¾è½¬æ¢ Pipeline
# ==========================================================
numeric_transformer = Pipeline(steps=[("scaler", StandardScaler())])
categorical_transformer = Pipeline(
    steps=[("onehot", OneHotEncoder(handle_unknown="ignore"))]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_cols),
        ("cat", categorical_transformer, categorical_cols),
    ]
)

# æ‹Ÿåˆ + è½¬æ¢
X_processed = preprocessor.fit_transform(pdf)
print(f"âœ… Feature shape after preprocessing: {X_processed.shape}")

# ä¿å­˜å¤„ç†å™¨
joblib.dump(preprocessor, os.path.join(MODEL_DIR, "preprocessor.pkl"))

# ==========================================================
# ğŸ§© Step 4. æ„å»º AutoEncoder æ¨¡å‹
# ==========================================================
input_dim = X_processed.shape[1]


class AutoEncoder(nn.Module):
    def __init__(self, input_dim, hidden_dim=128):
        super(AutoEncoder, self).__init__()
        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(hidden_dim, input_dim))

    def forward(self, x):
        return self.decoder(self.encoder(x))


model = AutoEncoder(input_dim=input_dim, hidden_dim=128)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

# ==========================================================
# ğŸ§© Step 5. åˆ’åˆ†æ•°æ®é›†å¹¶è®­ç»ƒ
# ==========================================================
X_train, X_val = train_test_split(X_processed, test_size=0.2, random_state=42)

# âœ… ç›´æ¥è½¬æ¢ä¸º tensorï¼ˆä¸éœ€è¦ toarrayï¼‰
X_train_tensor = torch.tensor(np.array(X_train), dtype=torch.float32)
X_val_tensor = torch.tensor(np.array(X_val), dtype=torch.float32)

epochs = 20
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    output = model(X_train_tensor)
    loss = criterion(output, X_train_tensor)
    loss.backward()
    optimizer.step()

    model.eval()
    with torch.no_grad():
        val_output = model(X_val_tensor)
        val_loss = criterion(val_output, X_val_tensor)

    print(
        f"Epoch [{epoch+1}/{epochs}] Train Loss={loss.item():.6f} Val Loss={val_loss.item():.6f}"
    )

# ==========================================================
# ğŸ§© Step 6. é˜ˆå€¼è®¡ç®—
# ==========================================================
model.eval()
with torch.no_grad():
    recon = model(X_train_tensor)
    errors = torch.mean((X_train_tensor - recon) ** 2, dim=1).numpy()
threshold = np.mean(errors) + 2 * np.std(errors)
print(f"âœ… Threshold calculated: {threshold:.6f}")

# ==========================================================
# ğŸ§© Step 7. æ¨¡å‹ä¿å­˜
# ==========================================================
torch.save(model.state_dict(), os.path.join(MODEL_DIR, "autoencoder.pth"))
joblib.dump(threshold, os.path.join(MODEL_DIR, "threshold.pkl"))
print(f"ğŸ¯ Model & threshold saved to {MODEL_DIR}")
