import time
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    get_json_object,
    regexp_extract,
    from_json,
    regexp_replace,
)
from pyspark.sql.types import StringType

# ==========================================================
# ğŸ§© Step 0. é…ç½®ä¸åˆå§‹åŒ–
# ==========================================================
TARGET_ROWS = 5000  # <--- ç›®æ ‡æ”¶é›†è¡Œæ•°
KAFKA_SERVERS = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"
PARQUET_PATH = "/opt/spark/work-dir/data/structured_data"
CHECKPOINT_PATH = "/opt/spark/work-dir/data/_checkpoints_structured_data"
# ä½¿ç”¨å…¨å±€å˜é‡åœ¨ Driver ç«¯è¿›è¡Œè®¡æ•°
global_counter = {"total_rows": 0}

spark = SparkSession.builder.appName("LogPreprocessingMultiSource").getOrCreate()
spark.sparkContext.setLogLevel("WARN")
print(f"âœ… Spark initialized. Target rows: {TARGET_ROWS}")


# ==========================================================
# ğŸ§© Step 1-3. åŸå§‹æ•°æ®å¤„ç†ä¸åˆå¹¶ (é€»è¾‘ä¸å˜ï¼Œä»…ç§»é™¤æœ«å°¾çš„ await)
# ==========================================================
df = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_SERVERS)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)
df = df.selectExpr("CAST(value AS STRING) as json_str")

# é€šç”¨å­—æ®µæå–
df_base = df.select(
    get_json_object(col("json_str"), "$.source_type").alias("source_type"),
    get_json_object(col("json_str"), "$.timestamp").alias("timestamp"),
    get_json_object(col("json_str"), "$.message").alias("message"),
)

# å„ç±»æ—¥å¿—åˆ†æµè§£æ
df_github_commits = df_base.filter(col("source_type") == "github_commits").select(
    col("source_type"),
    get_json_object(col("message"), "$.email").alias("commit_email"),
    get_json_object(col("message"), "$.author").alias("commit_author"),
    get_json_object(col("message"), "$.repository").alias("commit_repository"),
)

df_github_actions = df_base.filter(col("source_type") == "github_actions").select(
    col("source_type"),
    get_json_object(col("message"), "$.event").alias("action_event"),
    get_json_object(col("message"), "$.name").alias("action_name"),
    get_json_object(col("message"), "$.pipeline_file").alias("action_pipeline_file"),
    get_json_object(col("message"), "$.build_branch").alias("action_build_branch"),
    get_json_object(col("message"), "$.status").alias("action_status"),
    get_json_object(col("message"), "$.conclusion").alias("action_conclusion"),
    get_json_object(col("message"), "$.repository").alias("action_repository"),
)

df_public_cloud = df_base.filter(col("source_type") == "public_cloud").select(
    col("source_type"),
    get_json_object(col("message"), "$.event_name").alias("event_name"),
    get_json_object(col("message"), "$.username").alias("username"),
)

df_app_metrics = df_base.filter(col("source_type") == "app_container_metrics").select(
    col("source_type"),
    get_json_object(col("message"), "$.device").alias("device"),
    get_json_object(col("message"), "$.kind").alias("kind"),
    get_json_object(col("message"), "$.name").alias("name"),
    get_json_object(col("message"), "$.value").alias("value"),
)

df_app_logs = df_base.filter(col("source_type") == "app_container_log").select(
    col("source_type"),
    get_json_object(col("message"), "$.level").alias("log_level"),
    get_json_object(col("message"), "$.logger").alias("logger_name"),
    get_json_object(col("message"), "$.message").alias("log_message"),
    get_json_object(col("message"), "$.service_name").alias("service_name"),
)

df_app_heartbeat = df_base.filter(col("source_type") == "fastapi_status").select(
    col("source_type"),
    get_json_object(col("message"), "$.name").alias("container_name"),
    get_json_object(col("message"), "$.status").alias("container_status"),
    get_json_object(col("message"), "$.status_code").alias("container_status_code"),
    get_json_object(col("message"), "$.url").alias("container_url"),
    get_json_object(col("message"), "$.value").alias("container_value"),
    get_json_object(col("message"), "$.message").alias("container_message"),
)

df_nginx = df_base.filter(col("source_type") == "nginx_access").select(
    col("source_type"),
    get_json_object(col("message"), "$.remote_addr").alias("client_ip"),
    get_json_object(col("message"), "$.request_method").alias("request_method"),
    get_json_object(col("message"), "$.request_uri").alias("request_uri"),
    get_json_object(col("message"), "$.status").alias("response_status"),
    get_json_object(col("message"), "$.body_bytes_sent").alias("body_bytes_sent"),
    get_json_object(col("message"), "$.request_time").alias("request_time"),
    get_json_object(col("message"), "$.http_user_agent").alias("user_agent"),
)

df_nginx_error = df_base.filter(col("source_type") == "nginx_error").select(
    col("source_type"),
    regexp_extract("message", r"\[(\w+)\]", 1).alias("error_level"),
    regexp_extract("message", r": (.*)", 1).alias("error_detail"),
)

df_final = (
    df_github_commits.unionByName(df_github_actions, allowMissingColumns=True)
    .unionByName(df_public_cloud, allowMissingColumns=True)
    .unionByName(df_app_metrics, allowMissingColumns=True)
    .unionByName(df_app_logs, allowMissingColumns=True)
    .unionByName(df_nginx, allowMissingColumns=True)
    .unionByName(df_nginx_error, allowMissingColumns=True)
    .unionByName(df_app_heartbeat, allowMissingColumns=True)
)


# ==========================================================
# ğŸ§© Step 4. ForeachBatch è®¡æ•°ä¸è¾“å‡º (æ–°é€»è¾‘)
# ==========================================================


def write_and_count(batch_df, batch_id):
    """
    å¤„ç†æ¯ä¸ªå¾®æ‰¹æ¬¡ï¼šå†™å…¥ Parquetï¼Œå¹¶æ›´æ–°å…¨å±€è®¡æ•°å™¨ã€‚
    """
    global global_counter

    # 1. å†™å…¥ Parquet æ–‡ä»¶ (æ–‡ä»¶è¾“å‡º)
    batch_df.write.mode("append").format("parquet").partitionBy("source_type").save(
        PARQUET_PATH
    )

    # 2. æ§åˆ¶å°è¾“å‡º (æ¨¡æ‹ŸåŸå§‹ console æµç¨‹)
    # print(f"\n--- Batch {batch_id} ---")
    # batch_df.show(5, truncate=False)  # åªæ˜¾ç¤ºå‰ 5 è¡Œ

    # 3. æ›´æ–°å…¨å±€è®¡æ•°å™¨
    count = batch_df.count()
    global_counter["total_rows"] += count

    # 4. æ‰“å°è¿›åº¦
    print(
        f"| Processed {count} rows. Total: {global_counter['total_rows']}/{TARGET_ROWS} |"
    )

    # 5. æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    if global_counter["total_rows"] >= TARGET_ROWS:
        # æŠ›å‡ºå¼‚å¸¸ï¼Œè®©ä¸»å¾ªç¯æ•è·å¹¶åœæ­¢
        raise Exception("Target row count reached, initiating shutdown.")


# --- å¯åŠ¨æµå¼æŸ¥è¯¢ ---
query_stream = (
    df_final.writeStream.outputMode("append")
    .option("checkpointLocation", CHECKPOINT_PATH)
    .foreachBatch(write_and_count)
    .start()
)

# --- ä¸»çº¿ç¨‹ç›‘æ§ä¸ç»ˆæ­¢ ---
print(f"\nâ° Streaming query started. Monitoring total rows. Target: {TARGET_ROWS}...")

try:
    # ä½¿ç”¨ awaitAnyTermination(timeout) é¿å…æ— é™é˜»å¡ï¼Œå¹¶åœ¨å†…éƒ¨é€šè¿‡ Exception è§¦å‘ç»ˆæ­¢
    # å› ä¸º foreachBatch ä¸­çš„ Exception ä¼šä¼ é€’åˆ° Driver çº¿ç¨‹
    # å¦‚æœè¶…è¿‡ 10 å°æ—¶è¿˜æ²¡è·‘å®Œï¼Œä¹Ÿè‡ªåŠ¨é€€å‡º
    query_stream.awaitTermination(timeout=36000)

except Exception as e:
    # æ•è·æˆ‘ä»¬åœ¨ foreachBatch ä¸­æŠ›å‡ºçš„ "Target row count reached" å¼‚å¸¸
    if "Target row count reached" in str(e):
        print(f"\nğŸ›‘ Target row count ({TARGET_ROWS}) reached. Stopping query...")
        query_stream.stop()
    else:
        print(f"\nâš ï¸ Unexpected error during streaming: {e}. Stopping query...")
        query_stream.stop()

spark.stop()
print("âœ… Spark session terminated.")
