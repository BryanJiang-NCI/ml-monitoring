"""
Spark Streaming Inference (Semantic Pipeline + AutoEncoder)
============================================================
âœ… ä¸è¯­ä¹‰å‘é‡è½åœ°é€»è¾‘å®Œå…¨ä¸€è‡´ï¼ˆåŒæ ·çš„ json â†’ semantic_text â†’ encodeï¼‰
âœ… ä» Kafka å®æ—¶è¯»å– â†’ å‘é‡åŒ– â†’ ä½¿ç”¨è®­ç»ƒå¥½çš„ AutoEncoder æ¨ç†
âœ… æ‰“å°é¢„æµ‹ç»“æœï¼ˆå¯é€‰åŒæ—¶è½åœ° parquetï¼‰
âœ… å¼‚å¸¸æ£€æµ‹ç»“æœå†™å…¥æ–‡ä»¶ (/opt/spark/work-dir/data/anomaly.log)
============================================================
"""

import os
import json
import torch
import torch.nn as nn
import joblib
import numpy as np
from datetime import datetime
from sentence_transformers import SentenceTransformer
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, udf, get_json_object
from pyspark.sql.types import StringType

# ==========================================================
# ğŸ§© Step 0. è·¯å¾„é…ç½®
# ==========================================================
KAFKA_SERVERS = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"


BASE_DIR = "/opt/spark/work-dir"  # æ ¹ç›®å½•ç»Ÿä¸€å®šä¹‰
MODEL_DIR = os.path.join(BASE_DIR, "models", "autoencoder_tfidf_torch")

SCALER_FILE = os.path.join(MODEL_DIR, "scaler.pkl")
MODEL_FILE = os.path.join(MODEL_DIR, "autoencoder.pth")
THRESH_FILE = os.path.join(MODEL_DIR, "threshold.pkl")
ANOMALY_LOG_FILE = os.path.join(BASE_DIR, "data/anomaly.log")
MODEL_NAME = "all-MiniLM-L6-v2"

print(f"ğŸš€ Initializing SentenceTransformer: {MODEL_NAME}")
encoder = SentenceTransformer(MODEL_NAME)

# ==========================================================
# ğŸ§© Step 1. åŠ è½½ AutoEncoder æ¨¡å‹ä¸æ ‡å‡†åŒ–å™¨
# ==========================================================
scaler = joblib.load(SCALER_FILE)
threshold = float(joblib.load(THRESH_FILE))
input_dim = int(getattr(scaler, "mean_", np.zeros(384)).shape[0]) or 384


class AutoEncoder(nn.Module):
    def __init__(self, input_dim=384, hidden_dim=128):
        super().__init__()
        self.encoder = nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(hidden_dim, input_dim))

    def forward(self, x):
        return self.decoder(self.encoder(x))


model = AutoEncoder(input_dim=input_dim, hidden_dim=128)
model.load_state_dict(torch.load(MODEL_FILE, map_location="cpu"))
model.eval()
print(f"âœ… Model loaded (input_dim={input_dim}, threshold={threshold:.6f})")

# ==========================================================
# ğŸ§© Step 2. Spark åˆå§‹åŒ–
# ==========================================================
spark = (
    SparkSession.builder.appName("SemanticStreamingInference")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .config("spark.sql.execution.arrow.pyspark.enabled", True)
    .config("spark.driver.memory", "4g")
    .config("spark.executor.memory", "4g")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")
print("âœ… Spark initialized successfully.")

# ==========================================================
# ğŸ§© Step 3. ä» Kafka è¯»å–
# ==========================================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_SERVERS)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)
df_raw = df_kafka.selectExpr("CAST(value AS STRING) as message")


# ==========================================================
# ğŸ§© Step 4. JSON â†’ Semantic Sentenceï¼ˆä¿æŒä¸€è‡´ï¼‰
# ==========================================================
def json_to_semantic(text):
    try:
        data = json.loads(text)
        parts = []
        for k, v in data.items():
            if isinstance(v, dict):
                for kk, vv in v.items():
                    parts.append(f"{k}.{kk}: {vv}")
            else:
                parts.append(f"{k}: {v}")
        return ". ".join(parts)
    except Exception:
        return "[INVALID_JSON]"


semantic_udf = udf(json_to_semantic, StringType())
df_semantic = df_raw.withColumn("semantic_text", semantic_udf(col("message")))
df_semantic = df_semantic.withColumn(
    "source_type", get_json_object(col("message"), "$.source_type")
)
df_semantic = df_semantic.withColumn(
    "ingest_time", get_json_object(col("message"), "$.timestamp")
)


# ==========================================================
# ğŸ§© Step 5. å‘é‡åŒ– + AutoEncoder æ¨ç† + å¼‚å¸¸è®°å½•
# ==========================================================
def infer_semantic(text):
    try:
        emb = encoder.encode(text)
        emb_scaled = scaler.transform([emb]).astype(np.float32)
        xt = torch.tensor(emb_scaled)
        with torch.no_grad():
            recon = model(xt)
            mse = torch.mean((xt - recon) ** 2, dim=1).item()

        if mse > threshold * 1.5:
            label = "high_anomaly"
        elif mse > threshold:
            label = "low_anomaly"
        else:
            label = "normal"

        result = {
            "timestamp": datetime.utcnow().isoformat(),
            "prediction": label,
            "mse": round(mse, 6),
            "threshold": round(threshold, 6),
        }

        # --- âš ï¸ å¼‚å¸¸å†™å…¥æ–‡ä»¶ ---
        if label != "normal":
            with open(ANOMALY_LOG_FILE, "a") as f:
                f.write(json.dumps(result) + "\n")
            print(f"âš ï¸ Anomaly detected â†’ logged to {ANOMALY_LOG_FILE}: {result}")

        return json.dumps(result)

    except Exception as e:
        return json.dumps({"prediction": "error", "error": str(e)})


infer_udf = udf(infer_semantic, StringType())
df_pred = df_semantic.withColumn("result", infer_udf(col("semantic_text")))

# ==========================================================
# ğŸ§© Step 6. è¾“å‡ºç»“æœ
# ==========================================================
query_console = (
    df_pred.select("source_type", "ingest_time", "semantic_text", "result")
    .writeStream.outputMode("append")
    .format("console")
    .option("truncate", False)
    .option("numRows", 5)
    .start()
)

# --- ç­‰å¾…ä»»åŠ¡ ---
print(f"ğŸ“¡ Streaming inference started from Kafka topic: {KAFKA_TOPIC}")
query_console.awaitTermination()
