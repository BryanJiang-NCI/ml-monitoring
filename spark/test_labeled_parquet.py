import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, get_json_object, regexp_extract, lower, when, lit

# ============================================
# âš™ï¸ å‚æ•°é…ç½®
# ============================================
KAFKA_BOOTSTRAP = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"

OUTPUT_PATH = "/opt/spark/work-dir/data/test_labeled_parquet"
CHECKPOINT_PATH = "/opt/spark/work-dir/data/_checkpoints_test_labeled_parquet"

os.makedirs(OUTPUT_PATH, exist_ok=True)

# ============================================
# ðŸš€ åˆå§‹åŒ– Spark
# ============================================
spark = (
    SparkSession.builder.appName("KafkaToLabeledParquet")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

print("âœ… Spark initialized: Kafka â†’ Labeled Parquet")

# ============================================
# ðŸ“¥ Kafka æµ
# ============================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

df_raw = df_kafka.selectExpr("CAST(value AS STRING) AS json_str")

# ============================================
# ðŸ“Œ Step 1: è§£æž JSON åŸºç¡€å­—æ®µ
# ============================================
df = df_raw.select(
    col("json_str"),
    get_json_object(col("json_str"), "$.source_type").alias("source_type"),
    get_json_object(col("json_str"), "$.message").alias("message"),
)

# ============================================
# ðŸ”§ Step 2: æå–ç”¨äºŽæ ‡ç­¾åˆ¤æ–­çš„å­—æ®µï¼ˆä»…åœ¨å¯¹åº” source_type ä¸‹è§£æžï¼‰
# ============================================

# msg_lowerï¼ˆæ‰€æœ‰æ—¥å¿—éƒ½å¯ä»¥å®‰å…¨è§£æžï¼‰
df = df.withColumn("msg_lower", lower(col("message")))

# -------- Nginx status only when source_type == nginx_access --------
df = df.withColumn(
    "nginx_status_raw",
    when(
        col("source_type") == "nginx_access",
        regexp_extract(col("message"), r"status[=: ](\d+)", 1),
    ).otherwise(""),
)

df = df.withColumn(
    "nginx_status",
    when(col("nginx_status_raw") != "", col("nginx_status_raw").cast("int")).otherwise(
        None
    ),
)


# ============================================
# ðŸ“Œ Step 3: è‡ªåŠ¨è§„åˆ™æ‰“æ ‡ç­¾ label
# ============================================
df = df.withColumn(
    "label",
    when(
        # 1. app log error
        (col("source_type") == "app_container_log")
        & (
            col("msg_lower").contains("error")
            | col("msg_lower").contains("unreachable")
            | col("msg_lower").contains("Error")
        ),
        1,
    )
    .when(
        # 2. nginx 5xx
        (col("source_type") == "nginx_access") & (col("nginx_status") >= 500),
        1,
    )
    .otherwise(0),
)

# ============================================
# ðŸ’¾ Step 4: å†™å…¥ Parquet
# ============================================
query = (
    df.writeStream.outputMode("append")
    .format("parquet")
    .option("path", OUTPUT_PATH)
    .option("checkpointLocation", CHECKPOINT_PATH)
    .partitionBy("source_type")
    .trigger(processingTime="20 seconds")
    .start()
)

print(f"ðŸ“¡ Kafka stream started â†’ {OUTPUT_PATH}")
query.awaitTermination()
