import os
import time
import requests
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, get_json_object, regexp_extract, lower, when
from pyspark.sql.types import IntegerType

# ============================================
# âš™ï¸ å‚æ•°é…ç½®
# ============================================
KAFKA_BOOTSTRAP = "kafka-kraft:9092"
KAFKA_TOPIC = "monitoring-data"
TARGET_ROWS = 1000  # ç›®æ ‡æ”¶é›†è¡Œæ•°
OUTPUT_PATH = "/opt/spark/work-dir/data/test_labeled_parquet"
CHECKPOINT_PATH = "/opt/spark/work-dir/data/_checkpoints_test_labeled_parquet"

# ============================================
# âš™ï¸ å¼‚å¸¸æ³¨å…¥é…ç½® (ç²¾ç®€)
# ============================================
MAX_INJECTION_COUNT = 25
API_ENDPOINT = "http://nginx/error"  # ä½ çš„å®é™…æ¥å£åœ°å€
DELAY_BETWEEN_CALLS = 0.1  # æ¯æ¬¡è°ƒç”¨ä¹‹é—´æš‚åœçš„ç§’æ•°

# ä½¿ç”¨å…¨å±€å˜é‡åœ¨ Driver ç«¯è¿›è¡Œè®¡æ•°
global_counter = {"total_rows": 0}
os.makedirs(OUTPUT_PATH, exist_ok=True)


# ============================================
# å¤–éƒ¨æ¥å£è°ƒç”¨å‡½æ•° (ç²¾ç®€)
# ============================================
def call_anomaly_injection_api(api_url: str) -> bool:
    """
    è°ƒç”¨å¤–éƒ¨æ¥å£ï¼Œè§¦å‘ä¸€æ¬¡å¼‚å¸¸ç”Ÿæˆï¼ˆå‡è®¾æ¥å£è‡ªè¡Œæ§åˆ¶ç”Ÿæˆçš„æ—¥å¿—æ•°é‡ï¼‰ã€‚
    """
    try:
        # ä½¿ç”¨ GET è¯·æ±‚ï¼Œä¸æºå¸¦ payload
        response = requests.get(api_url, timeout=10)
        print(response.text)
        response.raise_for_status()
        return True
    except requests.exceptions.RequestException:
        # å¿½ç•¥è¯¦ç»†é”™è¯¯ï¼Œè¿”å›å¤±è´¥çŠ¶æ€
        return False


# ============================================
# ğŸš€ æ‰¹é‡å¼‚å¸¸æ³¨å…¥é€»è¾‘ (ç²¾ç®€)
# ============================================
def run_injection_before_stream():
    """
    ä¸€æ¬¡æ€§æ‰§è¡Œ 50 æ¬¡æ¥å£è°ƒç”¨ï¼Œå°†å¼‚å¸¸æ•°æ®æ¨é€åˆ° Kafkaã€‚
    """
    print(f"==================================================")
    print(f"ğŸš€ å¯åŠ¨æ‰¹é‡å¼‚å¸¸æ³¨å…¥ (ç›®æ ‡: {MAX_INJECTION_COUNT} æ¬¡è°ƒç”¨)...")
    print(f"   API: {API_ENDPOINT}")

    start_time = time.time()

    for i in range(MAX_INJECTION_COUNT):
        call_anomaly_injection_api(API_ENDPOINT)

        # æ¯æ¬¡è°ƒç”¨ä¹‹é—´æš‚åœ
        time.sleep(DELAY_BETWEEN_CALLS)

    print(f"ğŸ‰ æ‰¹é‡æ³¨å…¥å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}s")
    print(f"==================================================")


# ============================================
# ğŸš€ åˆå§‹åŒ– Spark
# ============================================
spark = (
    SparkSession.builder.appName("KafkaToLabeledParquetAutoStop")
    .config("spark.sql.streaming.forceDeleteTempCheckpointLocation", True)
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")
print(f"âœ… Spark initialized. Target rows: {TARGET_ROWS}")


# ============================================
# ğŸ“Œ Step 1-3: æ•°æ®å¤„ç†ä¸è‡ªåŠ¨æ‰“æ ‡ç­¾
# ============================================
df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP)
    .option("subscribe", KAFKA_TOPIC)
    .option("startingOffsets", "latest")
    .load()
)

df_raw = df_kafka.selectExpr("CAST(value AS STRING) AS json_str")

df = df_raw.select(
    col("json_str"),
    get_json_object(col("json_str"), "$.source_type").alias("source_type"),
    get_json_object(col("json_str"), "$.message").alias("message"),
)

df = df.withColumn("msg_lower", lower(col("message")))

df_labeled = df.withColumn(
    "label",
    when(
        # 1. app log error
        (col("source_type") == "app_container_log")
        & (
            col("msg_lower").contains("error")
            | col("msg_lower").contains("unreachable")
            | col("msg_lower").contains("ERROR")
        ),
        1,
    )
    .when(
        # 2. nginx 5xx
        (col("source_type") == "nginx_access")
        & (col("msg_lower").contains('"status":500')),
        1,
    )
    .otherwise(0),  # æ­£å¸¸ -> 0
).drop("msg_lower")


# ============================================
# ğŸ’¾ Step 4: ForeachBatch è®¡æ•°ä¸è¾“å‡º
# ============================================
def write_and_count(batch_df, batch_id):
    """
    å¤„ç†æ¯ä¸ªå¾®æ‰¹æ¬¡ï¼šå†™å…¥ Parquetï¼Œå¹¶æ›´æ–°å…¨å±€è®¡æ•°å™¨ã€‚
    """
    global global_counter

    # å†™å…¥ Parquet æ–‡ä»¶
    batch_df.select(
        col("source_type"),
        col("message"),
        col("json_str"),
        col("label").cast(IntegerType()),
    ).write.mode("append").format("parquet").partitionBy("source_type").save(
        OUTPUT_PATH
    )

    # æ›´æ–°å…¨å±€è®¡æ•°å™¨
    count = batch_df.count()
    global_counter["total_rows"] += count

    # æ‰“å°è¿›åº¦
    print(
        f"| Batch {batch_id}: Processed {count} rows. Total: {global_counter['total_rows']}/{TARGET_ROWS} |"
    )

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    if global_counter["total_rows"] >= TARGET_ROWS:
        # æŠ›å‡ºå¼‚å¸¸ï¼Œè§¦å‘ä¸»çº¿ç¨‹ç»ˆæ­¢
        raise Exception("Target row count reached, initiating shutdown.")


# ============================================
# âš™ï¸ å¯åŠ¨ä¸»ç¨‹åº
# ============================================
if __name__ == "__main__":

    # 1. å¯åŠ¨æµå¼æŸ¥è¯¢ï¼ˆå…ˆå¯åŠ¨ï¼ï¼‰
    query = (
        df_labeled.writeStream.outputMode("append")
        .option("checkpointLocation", CHECKPOINT_PATH)
        .foreachBatch(write_and_count)
        .start()
    )

    print(f"ğŸ“¡ Kafka stream started â†’ {OUTPUT_PATH}. Waiting for {TARGET_ROWS} rows...")

    # â­ ç»™ Spark å‡ ç§’é’Ÿæ—¶é—´å»ºç«‹ Kafka è¿æ¥
    time.sleep(3)

    # 2. å¯åŠ¨æ‰¹é‡å¼‚å¸¸æ³¨å…¥ï¼ˆæ­¤æ—¶ Kafka â†’ Spark æµå·²ç»åœ¨å·¥ä½œï¼‰
    run_injection_before_stream()

    try:
        # é˜»å¡ä¸»çº¿ç¨‹ï¼Œç›´åˆ°è¾¾åˆ°ç›®æ ‡æ¡æ•°ï¼ˆåœ¨ foreachBatch ä¸­è§¦å‘ï¼‰
        query.awaitTermination(timeout=36000)

    except Exception as e:
        if "Target row count reached" in str(e):
            print(f"\nğŸ›‘ Target row count ({TARGET_ROWS}) reached. Stopping query...")
            query.stop()
        else:
            print(f"\nâš ï¸ Unexpected error during streaming: {e}. Stopping query...")
            query.stop()

    spark.stop()
    print("âœ… Spark session terminated.")
