#!/bin/bash
# ==============================================================
# ğŸš€ Generic Spark Submit Script
# Author: Bryan
# Description:
#   Run any Spark job by specifying the Python script name only.
#   It will automatically install Python dependencies from
#   /opt/spark/requirements.txt if available.
#   Example: bash run_spark.sh semantic_train.py
# ==============================================================

# === å‚æ•°æ£€æŸ¥ ===
if [ -z "$1" ]; then
  echo "âŒ Usage: bash run_spark.sh <script_name.py>"
  echo "Example: bash run_spark.sh semantic_train.py"
  exit 1
fi

SCRIPT_NAME=$1
APP_PATH="/opt/spark/${SCRIPT_NAME}"   # æˆ– /opt/spark/apps/${SCRIPT_NAME}ï¼ŒæŒ‰ä½ çš„ç›®å½•ç»“æ„ä¿®æ”¹
SPARK_MASTER_URL="spark://spark-master:7077"
SPARK_BIN="/opt/bitnami/spark/bin/spark-submit"
REQ_FILE="/opt/spark/requirements.txt"

echo ""
echo "ğŸš€ Submitting Spark job for ${SCRIPT_NAME} ..."
echo "----------------------------------------------"

# === Step 1. è‡ªåŠ¨å®‰è£…ä¾èµ– ===
if [ -f "${REQ_FILE}" ]; then
  echo "ğŸ“¦ Installing Python dependencies from ${REQ_FILE} ..."
  pip install -r ${REQ_FILE} --no-cache-dir --root-user-action=ignore
  if [ $? -ne 0 ]; then
    echo "âš ï¸ Dependency installation failed, continuing anyway..."
  else
    echo "âœ… Dependencies installed successfully."
  fi
else
  echo "â„¹ï¸ No requirements.txt found â€” skipping dependency installation."
fi

# === Step 2. æ£€æŸ¥ Spark è„šæœ¬æ˜¯å¦å­˜åœ¨ ===
if [ ! -f "${APP_PATH}" ]; then
  echo "âŒ Spark app not found at: ${APP_PATH}"
  exit 1
fi

# === Step 3. æäº¤ Spark ä»»åŠ¡ ===
${SPARK_BIN} \
  --master ${SPARK_MASTER_URL} \
  --driver-memory 4G \
  --executor-memory 2G \
  --executor-cores 2 \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
  ${APP_PATH}

EXIT_CODE=$?

# === Step 4. ç»“æœè¾“å‡º ===
if [ ${EXIT_CODE} -eq 0 ]; then
  echo "âœ… Spark job ${SCRIPT_NAME} completed successfully."
else
  echo "âŒ Spark job ${SCRIPT_NAME} failed (exit code: ${EXIT_CODE})."
fi
