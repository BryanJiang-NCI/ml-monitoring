#!/bin/bash
# ==============================================================
# üöÄ Generic Spark Submit Script
# Author: Bryan
# Description:
#   Run any Spark job by specifying the Python script name only.
#   It will automatically install Python dependencies from
#   /opt/spark/work-dir/requirements.txt if available.
#   Example: bash run_spark.sh semantic_train.py
# ==============================================================

# check parameters
if [ -z "$1" ]; then
  echo "‚ùå Usage: bash run_spark.sh <script_name.py>"
  echo "Example: bash run_spark.sh semantic_train.py"
  exit 1
fi

SCRIPT_NAME=$1
APP_PATH="/opt/spark/work-dir/${SCRIPT_NAME}"
SPARK_MASTER_URL="spark://spark-master:7077"
SPARK_BIN="/opt/spark/bin/spark-submit"

echo ""
echo "üöÄ Submitting Spark job for ${SCRIPT_NAME} ..."
echo "----------------------------------------------"

# check the script file exists
if [ ! -f "${APP_PATH}" ]; then
  echo "‚ùå Spark app not found at: ${APP_PATH}"
  exit 1
fi

# submit to the spark cluster
${SPARK_BIN} \
  --master ${SPARK_MASTER_URL} \
  --driver-memory 512M \
  --executor-memory 512M \
  --executor-cores 1 \
  --conf spark.cores.max=5 \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
  ${APP_PATH}

EXIT_CODE=$?

# report the result
if [ ${EXIT_CODE} -eq 0 ]; then
  echo "‚úÖ Spark job ${SCRIPT_NAME} completed successfully."
else
  echo "‚ùå Spark job ${SCRIPT_NAME} failed (exit code: ${EXIT_CODE})."
fi
