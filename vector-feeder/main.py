import os, json, requests, boto3, asyncio
from datetime import datetime, timedelta
from fastapi import FastAPI

app = FastAPI(title="Vector-Feeder")

DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)


def append_to_file(path, data):
    """write a JSON line to a file"""
    with open(path, "a", encoding="utf-8") as f:
        f.write(json.dumps(data, ensure_ascii=False) + "\n")


async def fetch_github_commits(owner, repo):
    """read GitHub commits via REST API and store them as JSON lines"""
    headers = {
        "Accept": "application/vnd.github+json",
    }
    base_url = f"https://api.github.com/repos/{owner}/{repo}/commits"
    page = 1

    while True:
        r = requests.get(
            base_url,
            headers=headers,
            params={"per_page": 100, "page": page},
            timeout=10,
        )
        r.raise_for_status()
        commits = r.json()

        if not commits:
            break  # no more commits â†’ stop

        for c in commits:
            repo_name = f"{owner}/{repo}"
            data = {
                "type": "github_commit",
                "author": c["commit"]["author"]["name"],
                "email": c["commit"]["author"]["email"],
                "date": c["commit"]["author"]["date"],
                "repository": repo_name,
            }
            append_to_file(os.path.join(DATA_DIR, "github_commits.jsonl"), data)

        page += 1


async def fetch_github_actions(owner, repo):
    """read GitHub Actions workflow runs via REST API and store them as JSON lines"""
    headers = {
        "Accept": "application/vnd.github+json",
    }
    base_url = f"https://api.github.com/repos/{owner}/{repo}/actions/runs"

    page = 1
    while True:
        r = requests.get(
            base_url,
            headers=headers,
            params={"per_page": 100, "page": page},
            timeout=10,
        )
        r.raise_for_status()
        runs = r.json().get("workflow_runs", [])

        if not runs:
            break

        for run in runs:
            rid = run["id"]
            data = {
                "type": "github_action",
                "name": run.get("name"),
                "event": run.get("event"),
                "pipeline_file": run.get("path"),
                "build_branch": run.get("head_branch"),
                "status": run.get("status"),
                "actor": run["actor"]["login"] if run.get("actor") else None,
                "conclusion": run.get("conclusion"),
                "created_at": run.get("created_at"),
                "repository": f"{owner}/{repo}",
            }

            append_to_file(os.path.join(DATA_DIR, "github_actions.jsonl"), data)

        page += 1


async def fetch_fastapi_health():
    """request FastAPI health endpoint and store the result"""
    url = "http://fastapi-demo:8000"
    try:
        r = requests.get(url, timeout=2)
        status_code = r.status_code
        if 200 <= status_code < 400:
            status = "running"
            value = 1.0
            message = f"Service is healthy and responding with HTTP {status_code}"
        else:
            status = "http_error"
            value = 0.5
            message = f"Service responded with error HTTP {status_code}"

    except requests.exceptions.ConnectionError:
        status = "connection_failed"
        value = 0.0
        status_code = 0
        message = "Service unreachable â€” connection failed"
    except requests.exceptions.Timeout:
        status = "timeout"
        value = 0.0
        status_code = 408
        message = "Service timed out â€” no response within 2 seconds"
    except Exception as e:
        status = "critical_failure"
        value = 0.0
        status_code = 500
        message = f"Unexpected exception occurred: {str(e)}"

    data = {
        "type": "container_status",
        "name": "fastapi_health",
        "status": status,
        "status_code": status_code,
        "url": url,
        "value": value,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "message": message,
    }
    append_to_file(os.path.join(DATA_DIR, "fastapi_health.jsonl"), data)
    print(f"ðŸ©º FastAPI health check: {status} (code={status_code})")


async def simulate_workload():
    """simulate real HTTP requests to generate workload"""
    import random

    urls = random.sample(
        [
            "http://nginx/",
            "http://nginx/db_read",
            "http://nginx/user_login",
        ],
        k=3,
    )
    print("ðŸš€ Real request simulation started.")
    while True:
        try:
            for url in urls:
                status_code = 0
                try:
                    r = requests.get(url, timeout=2)
                    status_code = r.status_code
                except requests.exceptions.RequestException:
                    status_code = 0  # connection failure or timeout

                print(f"ðŸŒ Simulated {url} â†’ {status_code}")
                await asyncio.sleep(0.5)

        except Exception as e:
            print("âŒ Simulated request error:", e)
            await asyncio.sleep(3)


async def periodic_fetch():
    while True:
        try:
            await fetch_github_commits("BryanJiang-NCI", "ml-monitoring")
            await fetch_github_actions("BryanJiang-NCI", "ml-monitoring")
            print(f"[{datetime.utcnow().isoformat()}] âœ… Data fetched.")
        except Exception as e:
            print("âŒ Fetch error:", e)
        await asyncio.sleep(60)


async def fetch_fastapi_periodically():
    while True:
        try:
            await fetch_fastapi_health()
        except Exception as e:
            print("âŒ FastAPI health fetch error:", e)
        await asyncio.sleep(10)


@app.on_event("startup")
async def startup_event():
    asyncio.create_task(periodic_fetch())
    asyncio.create_task(fetch_fastapi_periodically())
    asyncio.create_task(simulate_workload())


@app.get("/")
def root():
    return {"message": "âœ… VectorFeeder running", "cached": len(cache)}
